Description: A master and pool of replicas for Ether Cattle

Parameters:
  DiskSize:
    Default: '250'
    Description: Size of each node's chaindata storage volume in GiB
    MinValue: '8'
    Type: Number
  PrunedDiskSize:
    Default: '250'
    Description: Size of a pruned volume
    MinValue: '8'
    Type: Number
  ReplicaImageAMI:
    Default: ""
    Description: Custom AMI to use for the replica servers, empty string for default AWS AMI image
    Type: String
  ReplicaDiskType:
    AllowedValues:
    - standard
    - gp2
    - gp3
    - st1
    - sc1
    Default: gp2
    Description: Replica storage volume type
    Type: String
  ReplicaServeHTTP:
    Description: Enable replica to serve RPC over HTTP
    Type: String
    Default: 'true'
    AllowedValues:
    - 'true'
    - 'false'
  ReplicaServeGraphQL:
    Description: Enable replica to serve GraphQL over HTTP
    Type: String
    Default: 'true'
    AllowedValues:
    - 'true'
    - 'false'
  ReplicaServeWebsockets:
    Description: Enable replica to serve Websockets
    Type: String
    Default: 'true'
    AllowedValues:
    - 'true'
    - 'false'
  S3GethBucketName:
    Default: ethercattle-binaries
    Type: String
    Description: The bucket containing EtherCattle Geth Binaries
  ECGethVersion:
    Default: v1.10.3-1
    Type: String
    Description: The Ether Cattle Geth Version to deploys
  MasterGethVersion:
    Default: ""
    Type: String
    Description: If set, an alternate version of Geth to use on masters. If not set, ECGethVersion will be deployed.
  InfrastructureStack:
    Type: String
    Description: The infrastructure stack this cluster connects to
  KeyName:
    Type: AWS::EC2::KeyPair::KeyName
    Description: The name of the SSH key pair allowed to SSH into the nodes
  KafkaBrokerURL:
    Type: String
    Description: The string of kafka brokers to connect to.
  KafkaTopic:
    Type: String
    Description: A name for the Kafka Topic between the master and replicas. This must be unique for each cluster.
  NetworkId:
    Type: String
    Description: An identifier for the network this cluster represents. This should be common across all clusters representing the same network.
  SnapshotId:
    Type: String
    Description: A snapshot of the Ethereum folder with a synced blockchain
  MasterSize:
    Type: String
    Description: Whether to use full size masters or smaller ones. "full" will use a pool of large instances from the m5(ad) and r5(ad) families. "small" will use a pool of medium instances from the t3 and t3a families. For mainnet this must be full - for testnets and private networks it will depend on the network volume.
    AllowedValues:
      - full
      - medium
      - small
    Default: full
  MasterCount:
    Type: Number
    Description: The number of Geth masters to run with the cluster. More masters means higher availability and that replicas are likely to be updated faster, but higher replica startup times and more disk usage.
    Default: 1
  MasterExtraFlags:
    Type: String
    Description: Extra flags for the Geth master (mainly for running other than mainnet)
  MasterMemoryThresholdLong:
    Type: Number
    Default: 75
    Description: The amount of memory which should trigger alarms if used for > 30 minutes
  MasterMemoryThresholdLow:
    Type: Number
    Default: 10
    Description: The amount of memory below which an alarm should triger
  MasterMemoryThresholdHigh:
    Type: Number
    Default: 85
    Description: The amount of memory which should trigger alarms if exceeded for > 1 minute
  ReplicaExtraFlags:
    Type: String
    Description: Extra flags for the Geth replica (mainly for running other than mainnet)
  ReplicaTargetCapacity:
    Type: Number
    Default: 2
    Description: Minimum number of instances for replicas
  ReplicaMaxCapacity:
    Type: Number
    Default: 5
    Description: Maximum number of instances for replicas
  ReplicaMemoryThresholdHigh:
    Type: Number
    Default: 85
    Description: The amount of memory which should trigger alarms if exceeded for > 1 minute
  FallbackTargetCapacity:
    Type: Number
    Default: 0
    Description: Minimum number of instances for replicas
  FallbackMaxCapacity:
    Type: Number
    Default: 5
    Description: Maximum number of instances for replicas
  ReplicaOnDemandPercentage:
    Type: Number
    Default: 0
    Description: The percentage (0 - 100) of replica that should be on-demand instead of spot instances.
  FallbackOnDemandPercentage:
    Type: Number
    Default: 100
    Description: The percentage (0 - 100) of replica that should be on-demand instead of spot instances.
  ReplicaSpotAllocationStrategy:
    Type: String
    AllowedValues:
      - "lowest-price"
      - "capacity-optimized"
    Default: "capacity-optimized"
  ReplicaCPUScalingTargetValue:
    Type: Number
    Default: 80
    Description: The percentage (0 - 100) CPU utilization target for auto scaling replicas
  ReplicaSize:
    Type: String
    Description: Whether to use full size replicas or smaller ones. "full" will use a pool of large instances from the m5d, m5ad, r5d, and r5ad families. "small" will use a pool of medium instances from the t3 and t3a families. Use "small" if you expect a small request volume.
    AllowedValues:
      - full
      - medium
      - small
    Default: full
  MasterOnDemandPercentage:
    Type: Number
    Default: 50
    Description: The percentage (0 - 100) of masters that should be on-demand instead of spot instances.
  MasterSpotAllocationStrategy:
    Type: String
    AllowedValues:
      - "lowest-price"
      - "capacity-optimized"
    Default: "capacity-optimized"
  AlternateTargetGroup:
    Type: String
    Description: An alternative comma-separated list of target groups that replicas should be assigned to.
  NotificationEmail:
    Type: String
    Description: An optional e-mail address to receive notifications from alarms
  AlarmSNSTopic:
    Type: String
    Description: An optional SNS topic to receive notifications from alarms
  SnapshotValidationThreshold:
    Type: Number
    Default: 10000
    Description: The number of state trie nodes to validate when taking a snapshot.
  RemoteRPCURL:
    Type: String
    Description: A remote RPC URL to check against local block numbers. If provided, an alarm will go off if this cluster falls significantly behind the specified RPC endpoint. If not specified, an alarm will go off if no blocks are processed in a one minute period.
  ReplicaExtraSecurityGroup:
    Type: String
    Description: An additional security to be assigned to Replicas. Leave this blank unless you need to add additional connectivity rules.
  DashboardExtension:
    Type: String
    Description: The name of a cloudformation output to be added to the dashboard
  SnapshotScheduleExpression:
    Type: String
    Description: A schedule expression for the frequency to take snapshots
    Default: "cron(55 0/6 * * ? *)"
  EventsTopic:
    Type: String
    Description: A Kafka topic for log data and events
  FlumeURL:
    Type: String
    Description: (optional) The URL of a Flume server for indexed logging
  FreezerBucket:
    Type: String
    Description: (optional) An s3 bucket to use for storing Geth Freezer data. If not provided, the InfrastructureStack's bucket will be used
  EnableS3Freezer:
    Type: String
    Default: "false"
    AllowedValues:
      - "true"
      - "false"
  S3BackupLogsBucket:
    Type: String
    Description: A bucket that has already been created to store logs in. Will create a subirectory flume folder.
    Default: ""
  ReplicaClusterVersion:
    Description: A high level version number for the replica cluster. Used for blue / green deployments
    Default: "1"
    Type: String
  ConsulAccountName:
    Type: String
    Description: Datacenter prefix for consul
  ConsulEc2RetryTagKey:
    Description:
      The EC2 instance tag key to filter on when joining to other Consul
      nodes.
    Type: String
    Default: "rivet-consul-cluster"
    ConstraintDescription: Must match EC2 Tag Name requirements.
  ConsulEc2RetryTagValue:
    Description:
      The EC2 instance tag value to filter on when joining to other Consul
      nodes.
    Type: String
    Default: "rivet-consul-member"
    ConstraintDescription: Must match EC2 Tag Name requirements.
  ConsulPrimaryRegion:
    Description: The AWS region of the primary Consul cluster. If not set, defaults to this region.
    Type: String
  ConsulCIDRBlock:
    Description: The CIDR Block of Consul clients that may connect to these services.
    Default: 10.255.255.255/32
    Type: String
  UrgentAlarmWebhook:
    Description: URL for webhooks pertaining to urgent alarms
    Type: String
  WarningAlarmWebhook:
    Description: URL for webhooks pertaining to warning alarms
    Type: String
  PrunerGeneration:
    Description: How many times have the volumes on this stack been pruned (this is managed by the stack, and can be ignored)
    Default: 0
    Type: Number
  SnapshotTimestamp:
    Description: The time the last timestamp was taken (this is managed by the stack and can be ignored)
    Default: 0
    Type: Number
  SnapsdbSize:
    Description: The volume size for master snapshots. If 0, snapshots will not be enabled.
    Default: 0
    Type: Number
  PrunerInstanceType:
    Description: Instance type to be used for pruning
    AllowedValues:
    - i3en.large
    - i3en.xlarge
    - r5a.large,r5.large
    Default: r5a.large,r5.large
    Type: String
  SnapshotOffset:
    Description: Kafka offset to be used for resuming the current snapshot. This will be cleared when a snapshot is taken.
    Type: String
    Default: ""
  MSKClusterName:
    Description: If using MSK for Kafka, this will enable OffsetLag Alarms. Leave blank to disable alarms, or if not using MSK.
    Type: String
    Default: ""


Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: Infrastructure
        Parameters:
          - InfrastructureStack
          - EventsTopic
          - AlternateTargetGroup
          - AlarmSNSTopic
          - NotificationEmail
          - KeyName
          - RemoteRPCURL
          - SnapshotScheduleExpression
          - DashboardExtension
          - FlumeURL
          - FreezerBucket
          - EnableS3Freezer
          - MSKClusterName
      - Label:
          default: Cluster
        Parameters:
          - KafkaBrokerURL
          - KafkaTopic
          - NetworkId
          - S3GethBucketName
          - ECGethVersion
          - MasterGethVersion
          - SnapshotId
          - S3BackupLogsBucket
      - Label:
          default: Master
        Parameters:
          - MasterSize
          - MasterCount
          - MasterOnDemandPercentage
          - MasterSpotAllocationStrategy
          - MasterExtraFlags
          - DiskSize
          - MasterMemoryThresholdLong
          - MasterMemoryThresholdHigh
      - Label:
          default: Replica
        Parameters:
          - ReplicaSize
          - ReplicaImageAMI
          - ReplicaServeHTTP
          - ReplicaServeGraphQL
          - ReplicaServeWebsockets
          - ReplicaExtraFlags
          - ReplicaDiskType
          - ReplicaTargetCapacity
          - ReplicaMaxCapacity
          - ReplicaOnDemandPercentage
          - ReplicaSpotAllocationStrategy
          - SnapshotValidationThreshold
          - ReplicaExtraSecurityGroup
      - Label:
          default: Fallback
        Parameters:
          - FallbackTargetCapacity
          - FallbackMaxCapacity
          - FallbackOnDemandPercentage
    ParameterLabels:
      S3BackupLogsBucket:
        default: S3 logs backup bucket
      MasterSize:
        default: Master Size
      DiskSize:
        default: Disk Size
      ReplicaImageAMI:
        default: Replica AMI Image
      ReplicaServeHTTP:
        default: Enable Replica RPC HTTP server
      ReplicaServeGraphQL:
        default: Enable Replica GraphQL server
      ReplicaServeWebsockets:
        default: Enable Replica Websockets server
      ReplicaDiskType:
        default: Disk Type
      S3GethBucketName:
        default: S3 Geth Bucket
      ECGethVersion:
        default: Ether Cattle Geth Version Number
      MasterGethVersion:
        default: Ether Cattle Geth Version for Masters
      InfrastructureStack:
        default: Infrastructure CloudFormation Stack
      KeyName:
        default: SSH Key Pair
      KafkaBrokerURL:
        default: Kafka Broker URL
      KafkaTopic:
        default: Unique Kafka Topic Name
      NetworkId:
        default: Unique Network ID
      SnapshotId:
        default: Chaindata Snapshot ID
      MasterCount:
        default: Master Count
      MasterExtraFlags:
        default: Extra Geth Flags
      ReplicaExtraFlags:
        default: Extra Geth Flags
      ReplicaTargetCapacity:
        default: Target Capacity
      ReplicaOnDemandPercentage:
        default: On-Demand Percentage
      ReplicaSpotAllocationStrategy:
        default: Spot Allocation Strategy
      MasterSpotAllocationStrategy:
        default: Spot Allocation Strategy
      ReplicaExtraSecurityGroup:
        default: Replica Extra Security Group
      AlternateTargetGroup:
        default: Alternate Target Group
      NotificationEmail:
        default: Notification Email Address
      AlarmSNSTopic:
        default: SNS Topic for Alarms
      SnapshotValidationThreshold:
        default: Snapshot Validation Threshold
      RemoteRPCURL:
        default: Remote RPC URL
      DashboardExtension:
        default: Dashboard Extension
      SnapshotScheduleExpression:
        default: Snapshot Schedule Expression
      EventsTopic:
        default: Events topic
      FlumeURL:
        default: Flume URL
      FallbackOnDemandPercentage:
        default: Fallback On Demand Percentage
      FreezerBucket:
        default: Freezer S3 Bucket
      MSKClusterName:
        default: MSK Cluster Name

Mappings:
  InstanceSizes:
    Master:
      full:
        - InstanceType: m4.xlarge
        - InstanceType: m5a.xlarge
        - InstanceType: m5ad.xlarge
        - InstanceType: m5.xlarge
        - InstanceType: m5d.xlarge
        - InstanceType: m5dn.xlarge
        - InstanceType: r5.large
        - InstanceType: r5d.large
        - InstanceType: r5dn.large
        - InstanceType: r5a.large
        - InstanceType: r5ad.large
      medium:
        - InstanceType: m6i.large
        - InstanceType: m5.large
        - InstanceType: m5a.large
        - InstanceType: m5ad.large
        - InstanceType: m5d.large
        - InstanceType: m5dn.large
      small:
        - InstanceType: t3.medium
        - InstanceType: t3a.medium
    Replica:
      full:
        - InstanceType: r5d.xlarge
        - InstanceType: r5dn.xlarge
        - InstanceType: r5ad.xlarge
        - InstanceType: r5.xlarge
        - InstanceType: r5n.xlarge
        - InstanceType: r5a.xlarge
        - InstanceType: r5b.xlarge
        # - InstanceType: c5d.xlarge
        # - InstanceType: c5ad.2xlarge
      medium:
        - InstanceType: m6i.large
        - InstanceType: m5.large
        - InstanceType: m5a.large
        - InstanceType: m5ad.large
        - InstanceType: m5d.large
        - InstanceType: m5dn.large
      small:
        - InstanceType: t3.medium
        - InstanceType: t3a.medium
    Fallback:
      full:
        # - InstanceType: i3.2xlarge
        - InstanceType: c5.4xlarge
        - InstanceType: c5d.4xlarge
        - InstanceType: m5d.4xlarge
        - InstanceType: m5.4xlarge
        # - InstanceType: c5d.xlarge
        # - InstanceType: c5ad.2xlarge
      medium:
        - InstanceType: m5ad.xlarge
        - InstanceType: m5d.xlarge
      small:
        - InstanceType: m5ad.xlarge
        - InstanceType: m5d.xlarge
  PoolSize:
    Size:
      full: 11
      medium: 6
      small: 6
  RegionMap:
    us-west-1:
      AL2AMI: ami-056ee704806822732
    eu-central-1:
      AL2AMI: ami-0cc293023f983ed53
    cn-north-1:
      AL2AMI: ami-0cad3dea07a7c36f9
    us-east-1:
      AL2AMI: ami-0b898040803850657
    ap-northeast-2:
      AL2AMI: ami-095ca789e0549777d
    us-gov-west1:
      AL2AMI:  ami-6b157f0a
    sa-east-1:
      AL2AMI: ami-058943e7d9b9cabfb
    ap-northeast-3:
      AL2AMI: ami-088d713d672ed235e
    ap-northeast-1:
      AL2AMI: ami-0c3fd0f5d33134a76
    ap-southeast-1:
      AL2AMI: ami-01f7527546b557442
    us-east-2:
      AL2AMI: ami-0d8f6eb4f641ef691
    ap-southeast-2:
      AL2AMI: ami-0dc96254d5535925f
    cn-northwest-1:
      AL2AMI: ami-094b7433620966eb5
    eu-west-1:
      AL2AMI: ami-0bbc25e23a7640b9b
    eu-north-1:
      AL2AMI: ami-d16fe6af
    us-gov-east1:
      AL2AMI: ami-1208ee63
    ap-south-1:
      AL2AMI: ami-0d2692b6acea72ee6
    eu-west-3:
      AL2AMI: ami-0adcddd3324248c4c
    eu-west-2:
      AL2AMI: ami-0d8e27447ec2c8410
    ca-central-1:
      AL2AMI: ami-0d4ae09ec9361d8ac
    us-west-2:
      AL2AMI: ami-082b5a644766e0e6f

Conditions:
  HasKeyName: !Not [!Equals [!Ref KeyName, '']]
  HasATG: !Not [!Equals [!Ref AlternateTargetGroup, '']]
  NoTG: !Equals [!Ref AlternateTargetGroup, 'NONE']
  ReplicaHDD: !Or [!Equals [ !Ref ReplicaDiskType, "st1"], !Equals [ !Ref ReplicaDiskType, "sc1"]]
  SmallDisk: !Or [
      !Equals [ !Ref DiskSize, "75" ],
      !Equals [ !Ref DiskSize, "200" ],
      !Equals [ !Ref DiskSize, "250" ],
      !Equals [ !Ref DiskSize, "300" ],
      !Equals [ !Ref DiskSize, "350" ],
      !Equals [ !Ref DiskSize, "400" ],
      !Equals [ !Ref DiskSize, "450" ],
    ]
  HasNotificationEmail: !Not [!Equals [ !Ref NotificationEmail, "" ]]
  HasSNSTopic: !Not [!Equals [ !Ref AlarmSNSTopic, "" ]]
  HasReplicaHTTP: !Equals
    - !Ref ReplicaServeHTTP
    - 'true'
  HasReplicaGraphQL: !Equals
    - !Ref ReplicaServeGraphQL
    - 'true'
  HasReplicaWebsockets: !Equals
    - !Ref ReplicaServeWebsockets
    - 'true'
  HasReplicaImageAMI: !Not [!Equals [ !Ref ReplicaImageAMI, "" ]]
  HasRemoteRPCURL: !Not [!Equals [!Ref RemoteRPCURL, ""]]
  NoRemoteRPCURL: !Equals [ !Ref RemoteRPCURL, ""]
  HasExtraSecurityGroup: !Not [!Equals [ !Ref ReplicaExtraSecurityGroup, "" ]]
  SmallMaster: !Equals [ !Ref MasterSize, "small"]
  SmallReplica: !Equals [ !Ref ReplicaSize, "small"]
  HasDashboardExtensions: !Not [!Equals [ !Ref DashboardExtension, "" ]]
  HasEventsTopic: !Not [!Equals [!Ref EventsTopic, "" ]]
  HasFlume: !Not [!Equals [!Ref FlumeURL, ""]]
  MasterSpotLowestPrice: !Equals [!Ref MasterSpotAllocationStrategy, "lowest-price"]
  ReplicaSpotLowestPrice: !Equals [!Ref ReplicaSpotAllocationStrategy, "lowest-price"]
  HasFreezerBucket: !Not [ !Equals [ !Ref FreezerBucket, "" ] ]
  EnabledFreezerBucket: !Equals [ !Ref EnableS3Freezer, "true" ]
  SpecifiesConsulRegion: !Not [ !Equals [ !Ref ConsulPrimaryRegion, "" ] ]
  HasUrgentWebhook: !Not [ !Equals [ !Ref UrgentAlarmWebhook, "" ]]
  HasWarningWebhook: !Not [ !Equals [ !Ref WarningAlarmWebhook, "" ]]
  HasSnapsdb: !Not [ !Equals [ !Ref SnapsdbSize, "0"]]
  HasSnapshotOffset: !Not [ !Equals [ !Ref SnapshotOffset, ""]]
  HasMSKName: !Not [!Equals [ !Ref MSKClusterName, "" ]]

Resources:
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal: {Service: [lambda.amazonaws.com]}
          Action: ['sts:AssumeRole']
      Path: "/"
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
  MulMin:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        ZipFile: !Sub |
          var response = require('./cfn-response');
          exports.handler = function(event, context) {
            var result = parseInt(event.ResourceProperties.Op1) * parseInt(event.ResourceProperties.Op2);
            if(event.ResourceProperties.Max) {
              result = Math.min(result, parseInt(event.ResourceProperties.Max));
            }
            response.send(event, context, response.SUCCESS, {Value: result});
          };
      Runtime: nodejs12.x
  Max:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        ZipFile: !Sub |
          var response = require('./cfn-response');
          exports.handler = function(event, context) {
            var result = Math.max(parseInt(event.ResourceProperties.Op1), parseInt(event.ResourceProperties.Op2));
            response.send(event, context, response.SUCCESS, {Value: result});
          };
      Runtime: nodejs12.x
  Sum:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        ZipFile: !Sub |
          var response = require('./cfn-response');
          exports.handler = function(event, context) {
            response.send(event, context, response.SUCCESS, {Sum: event.ResourceProperties.Values.reduce((a,b) => parseInt(a)+parseInt(b))});
          };
      Runtime: nodejs12.x
  NextPrunerGeneration:
    Type: Custom::Sum
    Properties:
      ServiceToken: !GetAtt Sum.Arn
      Values:
        - !Ref PrunerGeneration
        - 1
  HDDSize:
    Type: Custom::Max
    Properties:
      ServiceToken: !GetAtt Max.Arn
      Op1: !Ref DiskSize
      Op2: 500
  VolumeIOPS:
    Type: Custom::MulMin
    Properties:
      ServiceToken: !GetAtt MulMin.Arn
      Op1: !Ref DiskSize
      Op2: 50
      Max: 5000
  MasterLG:
    Type: AWS::Logs::LogGroup
    Properties:
      RetentionInDays: 7
      LogGroupName:
        "Fn::Sub":
          - "/${ClusterId}/${AWS::StackName}/master"
          - ClusterId:
              "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
  MasterNodeSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Allow internal SSH access and ETH p2p connectivity
      VpcId:
        "Fn::ImportValue": !Sub "${InfrastructureStack}-VpcId"
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: '22'
        ToPort: '22'
        CidrIp: !Join ["", ["Fn::ImportValue": !Sub "${InfrastructureStack}-VpcBaseIp", ".0.0/14"]]
      - IpProtocol: udp
        FromPort: '30303'
        ToPort: '30303'
        CidrIp: '0.0.0.0/0'
      - IpProtocol: tcp
        FromPort: '30303'
        ToPort: '30303'
        CidrIp: '0.0.0.0/0'
      - IpProtocol: udp
        FromPort: '30301'
        ToPort: '30301'
        CidrIp: '0.0.0.0/0'
  MasterNodeRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - ec2.amazonaws.com
            - autoscaling.amazonaws.com
        Version: '2012-10-17'
  MasterNodePolicy:
    Type: "AWS::IAM::Policy"
    Properties:
      Roles:
        - !Ref MasterNodeRole
      PolicyName: !Sub "MasterNode${KafkaTopic}"
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Action:
              - logs:CreateLogStream
              - logs:CreateLogGroup
              - logs:PutLogEvents
            Effect: Allow
            Resource: "*"
            Sid: Stmt3
          - Action:
              - s3:GetObject
              - s3:ListBucket
              - s3:GetBucketPolicy
              - s3:GetObjectTagging
              - s3:GetBucketLocation
            Resource: !Sub arn:aws:s3:::${S3GethBucketName}/*
            Effect: Allow
          - Action:
              - s3:GetObject
              - s3:PutObject
              - s3:ListBucket
              - s3:GetBucketPolicy
              - s3:GetObjectTagging
              - s3:GetBucketLocation
            Resource:
              - {"Fn::Sub": ["arn:aws:s3:::${Bucket}/*", {"Bucket": !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]}]}
              - {"Fn::Sub": ["arn:aws:s3:::${Bucket}", {"Bucket": !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]}]}
            Effect: Allow
          - Action:
              - cloudwatch:PutMetricData
              - ec2:DescribeTags
              - logs:PutLogEvents
              - logs:DescribeLogStreams
              - logs:DescribeLogGroups
              - logs:CreateLogStream
              - logs:CreateLogGroup
            Resource: "*"
            Effect: Allow
          - Action:
              - ssm:GetParameter
            Resource: !Sub "arn:aws:ssm:*:*:parameter/${MetricsConfigParameter}"
            Effect: Allow
          - Action:
              - ec2:ModifyVolume
              - ec2:DescribeVolumes
            Effect: Allow
            Resource: "*"
  MasterNodeInstanceProfile:
    Type: "AWS::IAM::InstanceProfile"
    Properties:
      Path: /
      Roles:
      - !Ref MasterNodeRole
    DependsOn: MasterNodeRole
  MetricsConfigParameter:
    Type: "AWS::SSM::Parameter"
    Properties:
      Type: String
      Value: '{"metrics":{"append_dimensions":{"AutoScalingGroupName":"${aws:AutoScalingGroupName}"},"metrics_collected":{"cpu":{"measurement":["cpu_usage_idle"],"metrics_collection_interval":60,"resources":["*"],"totalcpu":false},"disk":{"measurement":["used_percent"],"drop_device":true,"metrics_collection_interval":60,"resources":["/var/lib/ethereum","/var/lib/ethereum/overlay","/"]},"mem":{"measurement":["mem_used_percent"],"metrics_collection_interval":60},"statsd":{"metrics_aggregation_interval":60,"metrics_collection_interval":10,"service_address":":8125"},"swap":{"measurement":["swap_used_percent"],"metrics_collection_interval":60}}}}'

  MasterLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate

    Metadata:
      AWS::CloudFormation::Init:
          configSets:
            setup:
              - setup_ethercattle_files

          setup_ethercattle_files:
            files:
              /root/configure-server.sh:
                content:
                  "Fn::Sub":
                    - |
                      #!/bin/bash -xe
                      yum install -y aws-cfn-bootstrap

                      if [ "$(arch)" == "x86_64" ]
                      then
                        ARCH="amd64"
                      elif [ "$(arch)" == "aarch64" ]
                      then
                        ARCH="arm64"
                      fi

                      sysctl -p || true

                      curl "https://awscli.amazonaws.com/awscli-exe-linux-$(arch).zip" -o "awscliv2.zip"
                      unzip awscliv2.zip
                      ./aws/install

                      mkswap /dev/sds
                      swapon /dev/sds

                      GETH_BIN="geth-linux-$ARCH"
                      LOGS_BIN="journald-cloudwatch-logs-$ARCH"
                      aws s3 cp s3://${S3GethBucketName}/${ECGethVersion}/$GETH_BIN /usr/bin/geth
                      aws s3 cp s3://${S3GethBucketName}/$LOGS_BIN /usr/local/bin/journald-cloudwatch-logs
                      aws s3 cp s3://${S3GethBucketName}/peerManagerAuth.py /usr/local/bin/peerManager.py
                      chmod +x /usr/bin/geth
                      chmod +x /usr/local/bin/journald-cloudwatch-logs
                      chmod +x /usr/local/bin/peerManager.py
                      mkdir -p /var/lib/journald-cloudwatch-logs/
                      mkdir -p /var/lib/ethereum
                      mount -o barrier=0,data=writeback /dev/sdf /var/lib/ethereum
                      mkdir -p /var/lib/ethereum/overlay
                      resize2fs /dev/sdf
                      useradd -r geth

                      echo "/dev/sdf  /var/lib/ethereum    ext4   barrier=0,data=writeback,noatime  1   1" >> /etc/fstab

                      ignore="$(readlink -f /dev/sd*) $(readlink -f /dev/xvd*)"
                      cutignore="$(for x in $ignore ; do echo $x | cut -c -12; done | uniq)"
                      devices="$(ls /dev/nvme* | grep -E 'n1$')" || devices=""
                      cutdevices="$(for x in $devices ; do echo $x | cut -c -12; done | uniq)"
                      localnvme=$(for d in $cutdevices; do if ! $(echo "$cutignore"| grep -q $d) ; then echo $d; fi ; done)
                      if [ ! -z "$localnvme" ]
                      then
                        mkfs.ext4 $localnvme
                        mount -o barrier=0,data=writeback $localnvme /var/lib/ethereum/overlay
                        echo "$localnvme  /var/lib/ethereum/overlay    ext4   barrier=0,data=writeback,noatime  1   1" >> /etc/fstab
                      elif [ -e /dev/sdg ]
                      then
                        mkfs.ext4 /dev/sdg
                        mount -o barrier=0,data=writeback /dev/sdg /var/lib/ethereum/overlay
                        echo "/dev/sdg  /var/lib/ethereum/overlay    ext4   barrier=0,data=writeback,noatime  1   1" >> /etc/fstab
                      fi

                      if [ -e /dev/sdh ]
                      then
                        mkfs.ext4 /dev/sdh
                        mkdir -p /var/lib/ethereum/snapsdb
                        mount -o barrier=0,data=writeback /dev/sdh /var/lib/ethereum/snapsdb
                        echo "/dev/sdh  /var/lib/ethereum/snapsdb    ext4   barrier=0,data=writeback,noatime  1   1" >> /etc/fstab
                      fi

                      chown -R geth /var/lib/ethereum

                      yum install -y https://s3.amazonaws.com/amazoncloudwatch-agent/amazon_linux/$ARCH/latest/amazon-cloudwatch-agent.rpm nmap-ncat jq python-pip jq fio || true
                      pip install kafka-python
                      /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c ssm:${MetricsConfigParameter} -s

                      crontab -l >  newcrontab || true
                      echo "5,20,35,50 * * * * /usr/bin/sh -c 'for x in \$(ls /dev/sd*) ; do echo resizing \$(readlink -f \$x) if needed; /usr/sbin/resize2fs \$(readlink -f \$x) ; done'" >> newcrontab
                      crontab newcrontab


                      printf "[Unit]
                      Description=journald-cloudwatch-logs
                      Wants=basic.target
                      After=basic.target network.target

                      [Service]
                      ExecStart=/usr/local/bin/journald-cloudwatch-logs /usr/local/etc/journald-cloudwatch-logs.conf
                      KillMode=process
                      Restart=on-failure
                      RestartSec=42s" > /etc/systemd/system/journald-cloudwatch-logs.service

                      echo "geth        hard nofile 500000" >> /etc/security/limits.conf
                      echo "geth        soft nofile 500000" >> /etc/security/limits.conf

                      systemctl daemon-reload

                      sleep 5 #TODO- workaround for a deadlock on topic creation


                      systemctl enable amazon-cloudwatch-agent.service
                      systemctl start amazon-cloudwatch-agent.service
                      systemctl enable journald-cloudwatch-logs
                      systemctl start journald-cloudwatch-logs
                      # systemctl enable termination-detection.service
                      # systemctl start termination-detection.service
                    - ClusterId:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                      ReplicaHTTPFlag:
                          !If [HasReplicaHTTP, "--http --http.addr 0.0.0.0 --http.port 8545", ""]
                      ReplicaGraphQLFlag:
                          # Wow that's a lot of escaping!
                          !If [HasReplicaGraphQL, "--graphql --graphql.vhosts \\\\\\'*\\\\\\'", ""]
                      ReplicaWebsocketsFlag:
                          !If [HasReplicaWebsockets, "--ws --ws.addr 0.0.0.0 --ws.port 8546", ""]
                      BaseInfrastructure:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-BaseInfrastructure"
                      FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]
                      EventsTopicFlag: !If [ HasEventsTopic, !Sub "--kafka.event.topic ${EventsTopic}", ""]
                mode: "000700"
                owner: root
                group: root
              /root/service-configs.sh:
                content:
                  "Fn::Sub":
                    - |
                      #!/bin/bash -xe

                      SEP=$(echo "${KafkaBrokerURL}" | grep -q "?" && echo "&" || echo "?")
                      SEP_ESCAPED=$(echo "${KafkaBrokerURL}" | grep -q "?" && echo "\\&" || echo "?")
                      KAFKA_ESCAPED_URL="$(printf "${KafkaBrokerURL}"| sed 's^&^\\\\&^g')"

                      if [ -e /dev/sdg ]
                      then
                        OVERLAY_FLAG="--datadir.overlay=/var/lib/ethereum/overlay"
                      fi
                      ignore="$(readlink -f /dev/sd*) $(readlink -f /dev/xvd*)"
                      cutignore="$(for x in $ignore ; do echo $x | cut -c -12; done | uniq)"
                      devices="$(ls /dev/nvme* | grep -E 'n1$')" || devices=""
                      cutdevices="$(for x in $devices ; do echo $x | cut -c -12; done | uniq)"
                      localnvme=$(for d in $cutdevices; do if ! $(echo "$cutignore"| grep -q $d) ; then echo $d; fi ; done)
                      if [ ! -z "$localnvme" ]
                      then
                        OVERLAY_FLAG="--datadir.overlay=/var/lib/ethereum/overlay"
                      fi
                      if [ -e /dev/sdh ]
                      then
                        SNAPS_FLAG="--datadir.snaps=/var/lib/ethereum/snapsdb --snapshot=true"
                      else
                        SNAPS_FLAG="--snapshot=false --syncmode=full"
                      fi

                      totalm=$(free -m | awk '/^Mem:/{print $2}') ; echo $totalm
                      allocatesafe=$((totalm * 75 / 100))

                      printf "KafkaHostname=${KafkaBrokerURL}
                      KafkaTopic=${KafkaTopic}
                      NetworkId=${NetworkId}
                      infraName=${InfrastructureStack}
                      baseInfraName=${BaseInfrastructure}
                      network=${NetworkId}
                      ReplicaClusterVersion=${ReplicaClusterVersion}
                      AWS_REGION=${AWS::Region}
                      LOG_BLOCK_LIMIT=10000
                      FLUME_URL=${FlumeURL}" > /etc/systemd/system/ethcattle-vars


                      printf "[Unit]
                      Description=Ethereum go client replica
                      After=syslog.target network.target
                      [Service]
                      User=geth
                      Group=geth
                      Environment=HOME=/var/lib/ethereum
                      EnvironmentFile=/etc/systemd/system/ethcattle-vars
                      Type=simple
                      LimitNOFILE=655360
                      ExecStartPre=/usr/bin/bash -c '/usr/bin/geth replica --snapshot=false --cache=$allocatesafe ${ReplicaExtraFlags} ${FreezerFlags}  $OVERLAY_FLAG --kafka.broker=$KAFKA_ESCAPED_URL""$SEP_ESCAPED""fetch.default=8388608\\&max.waittime=25\\&avoid_leader=1  --datadir=/var/lib/ethereum --kafka.topic=${KafkaTopicString} --replica.syncshutdown 2>>/tmp/geth-stderr'
                      ExecStart=/usr/bin/bash -c '/usr/bin/geth replica --snapshot=false --cache=$allocatesafe ${ReplicaExtraFlags} ${FreezerFlags} $OVERLAY_FLAG --kafka.broker=$KAFKA_ESCAPED_URL --datadir=/var/lib/ethereum --kafka.topic=${KafkaTopicString} --kafka.txpool.topic=\$NetworkId-txpool  --kafka.tx.topic=\$NetworkId-tx --replica.startup.age=45  ${ReplicaHTTPFlag} ${ReplicaGraphQLFlag} ${ReplicaWebsocketsFlag}'
                      TimeoutStopSec=90
                      Restart=on-failure
                      TimeoutStartSec=86400
                      RestartSec=10s
                      [Install]
                      WantedBy=multi-user.target
                      " > /etc/systemd/system/geth-replica.service

                      printf "[Unit]
                      Description=Ethereum go client replica
                      After=syslog.target network.target
                      [Service]
                      User=geth
                      Group=geth
                      Environment=HOME=/var/lib/ethereum
                      EnvironmentFile=/etc/systemd/system/ethcattle-vars
                      Type=simple
                      LimitNOFILE=655360
                      # ExecStartPre=/usr/bin/geth replica --snapshot=false ${FreezerFlags}  $OVERLAY_FLAG  --cache=$allocatesafe ${FreezerFlags} --kafka.broker=$KAFKA_ESCAPED_URL""$SEP_ESCAPED""fetch.default=8388608\\&max.waittime=25\\&avoid_leader=1  --datadir=/var/lib/ethereum --kafka.topic=${KafkaTopicString} --replica.syncshutdown 2>>/tmp/geth-stderr || true
                      ExecStart=/usr/bin/bash -c '/usr/bin/geth ${FreezerFlags} ${MasterExtraFlags} --rpc.allow-unprotected-txs --snapshot=false --syncmode=full --txlookuplimit=0 $OVERLAY_FLAG  --cache=$allocatesafe ${FreezerFlags} --datadir=/var/lib/ethereum --light.maxpeers 0 --maxpeers 25 ${ReplicaHTTPFlag} ${ReplicaGraphQLFlag} ${ReplicaWebsocketsFlag}'
                      TimeoutStopSec=90
                      TimeoutStartSec=86400
                      RestartSec=10s
                      [Install]
                      WantedBy=multi-user.target
                      " > /etc/systemd/system/geth-fallback.service


                      printf "[Unit]
                      Description=Ethereum go client
                      After=syslog.target network.target

                      [Service]
                      User=geth
                      Group=geth
                      Environment=HOME=/var/lib/ethereum
                      EnvironmentFile=/etc/systemd/system/ethcattle-vars
                      Type=simple
                      LimitNOFILE=655360
                      ExecStart=/usr/bin/geth ${MasterExtraFlags} ${FreezerFlags} --rpc.allow-unprotected-txs $SNAPS_FLAG --txlookuplimit=0 --light.maxpeers 0 --maxpeers 25 --gcmode=archive --kafka.broker=${KafkaBrokerURL}""$SEP""net.maxopenrequests=1\&message.send.max.retries=20000  --datadir=/var/lib/ethereum --kafka.topic=${KafkaTopic} --kafka.txpool.topic=${NetworkId}-txpool ${EventsTopicFlag}
                      TimeoutStartSec=86400
                      TimeoutStopSec=90

                      [Install]
                      WantedBy=multi-user.target
                      " > /etc/systemd/system/geth-master.service

                      printf "export AWS_REGION=${AWS::Region}
                      export AWS_DEFAULT_REGION=${AWS::Region}
                      /usr/bin/bash -c '/usr/bin/geth replica --snapshot=false --cache=$allocatesafe ${ReplicaExtraFlags} ${FreezerFlags} --kafka.broker=$KAFKA_ESCAPED_URL""$SEP_ESCAPED""fetch.default=8388608\\&max.waittime=25\\&avoid_leader=1  --datadir=/var/lib/ethereum --kafka.topic=${KafkaTopicString} --replica.syncshutdown 2>>/tmp/geth-stderr'
                      chown -R geth /var/lib/ethereum" > /root/master-pre.sh

                      printf "[Unit]
                      Description=Ethereum go client
                      After=syslog.target network.target

                      [Service]
                      User=geth
                      Group=geth
                      Environment=HOME=/var/lib/ethereum
                      EnvironmentFile=/etc/systemd/system/ethcattle-vars
                      Type=simple
                      LimitNOFILE=655360
                      ExecStartPre=/usr/bin/bash -c '/usr/bin/geth replica --snapshot=false --cache=$allocatesafe ${ReplicaExtraFlags} ${FreezerFlags} $OVERLAY_FLAG --kafka.broker=$KAFKA_ESCAPED_URL""$SEP_ESCAPED""fetch.default=8388608\\&max.waittime=25\\&avoid_leader=1  --datadir=/var/lib/ethereum --kafka.topic=${KafkaTopicString} --replica.syncshutdown 2>>/tmp/geth-stderr'
                      ExecStart=/usr/bin/geth ${MasterExtraFlags} ${FreezerFlags} $OVERLAY_FLAG --rpc.allow-unprotected-txs $SNAPS_FLAG --txlookuplimit=0 --light.maxpeers 0 --maxpeers 25 --gcmode=archive --kafka.broker=${KafkaBrokerURL}""$SEP""net.maxopenrequests=1\&message.send.max.retries=20000  --datadir=/var/lib/ethereum --kafka.topic=${KafkaTopic} --kafka.txpool.topic=${NetworkId}-txpool ${EventsTopicFlag}
                      TimeoutStartSec=86400
                      TimeoutStopSec=90

                      [Install]
                      WantedBy=multi-user.target
                      " > /etc/systemd/system/geth-master-overlay.service


                      printf "[Unit]
                      Description=Ethereum go client transaction relay
                      After=syslog.target network.target geth

                      [Service]
                      User=geth
                      Group=geth
                      Environment=HOME=/var/lib/ethereum
                      Type=simple
                      ExecStart=/usr/bin/geth txrelay --kafka.broker=${KafkaBrokerURL} --kafka.tx.topic=${NetworkId}-tx --kafka.tx.consumergroup=${KafkaTopic}-cg /var/lib/ethereum/geth.ipc
                      TimeoutStopSec=90
                      Restart=on-failure
                      RestartSec=10s

                      [Install]
                      WantedBy=multi-user.target
                      " > /etc/systemd/system/geth-tx.service

                      printf "[Unit]
                      Description=Geth Peer Monitoring
                      After=syslog.target network.target geth

                      [Service]
                      User=geth
                      Group=geth
                      Environment=HOME=/var/lib/ethereum
                      Type=simple
                      ExecStart=/usr/local/bin/peerManager.py /var/lib/ethereum/geth.ipc ${NetworkId}-peerlist ${KafkaBrokerURL}
                      KillMode=process
                      TimeoutStopSec=90
                      Restart=on-failure
                      RestartSec=10s
                      " > /etc/systemd/system/geth-peer-data.service

                      printf "[Unit]
                      Description=Detect and shut down from termination signal
                      After=syslog.target

                      [Service]
                      User=root
                      Group=root
                      Type=simple
                      ExecStart=/root/termination-detection.sh
                      KillMode=process
                      TimeoutStopSec=90
                      Restart=on-failure
                      RestartSec=10s
                      " > /etc/systemd/system/termination-detection.service


                    - ClusterId:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                      ReplicaHTTPFlag:
                          !If [HasReplicaHTTP, "--http --http.addr 0.0.0.0 --http.port 8545", ""]
                      ReplicaGraphQLFlag:
                          # Wow that's a lot of escaping!
                          !If [HasReplicaGraphQL, "--graphql --graphql.vhosts \\\\\\'*\\\\\\'", ""]
                      ReplicaWebsocketsFlag:
                          !If [HasReplicaWebsockets, "--ws --ws.addr 0.0.0.0 --ws.port 8546", ""]
                      BaseInfrastructure:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-BaseInfrastructure"
                      FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]
                      EventsTopicFlag: !If [ HasEventsTopic, !Sub "--kafka.event.topic ${EventsTopic}", ""]
                      KafkaTopicString: !If [ HasSnapshotOffset, !Sub "${KafkaTopic}:${SnapshotOffset}", !Ref KafkaTopic ]
                mode: "000700"
                owner: root
                group: root
              /root/start-master.sh:
                content:
                  "Fn::Sub":
                    - |
                      #!/bin/bash -xe
                      printf "log_group = \"${MasterLG}\"
                      state_file = \"/var/lib/journald-cloudwatch-logs/state\"" > /usr/local/etc/journald-cloudwatch-logs.conf

                      if [ "${MasterGethVersion}" != "" ]
                      then
                        if [ "$(arch)" == "x86_64" ]
                        then
                          ARCH="amd64"
                        elif [ "$(arch)" == "aarch64" ]
                        then
                          ARCH="arm64"
                        fi
                        GETH_BIN="geth-linux-$ARCH"
                        aws s3 cp s3://${S3GethBucketName}/${MasterGethVersion}/$GETH_BIN /usr/bin/geth
                        chmod +x /usr/bin/geth
                      fi

                      rm -f /var/lib/ethereum/geth/nodekey || true
                      rm /var/lib/ethereum/geth.ipc || true
                      rm /etc/systemd/system/geth.service || true
                      ln -s /etc/systemd/system/geth-master.service /etc/systemd/system/geth.service
                      systemctl daemon-reload
                      systemctl enable geth-master.service
                      systemctl enable geth-tx.service
                      systemctl enable geth-peer-data.service

                      chmod +x /root/master-pre.sh
                      /root/master-pre.sh

                      systemctl start geth-master.service || true &
                      sleep 5
                      systemctl start geth-tx.service
                      systemctl start geth-peer-data.service

                    - ClusterId:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                      ReplicaHTTPFlag:
                          !If [HasReplicaHTTP, "--http --http.addr 0.0.0.0 --http.port 8545", ""]
                      ReplicaGraphQLFlag:
                          # Wow that's a lot of escaping!
                          !If [HasReplicaGraphQL, "--graphql --graphql.vhosts \\\\\\'*\\\\\\'", ""]
                      ReplicaWebsocketsFlag:
                          !If [HasReplicaWebsockets, "--ws --ws.addr 0.0.0.0 --ws.port 8546", ""]
                      BaseInfrastructure:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-BaseInfrastructure"
                      FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]
                      EventsTopicFlag: !If [ HasEventsTopic, !Sub "--kafka.event.topic ${EventsTopic}", ""]
                mode: "000700"
                owner: root
                group: root
              /root/start-replica.sh:
                content:
                  "Fn::Sub":
                    - |
                      #!/bin/bash -xe

                      printf "log_group = \"${ReplicaLG}\"\nstate_file = \"/var/lib/journald-cloudwatch-logs/state\"" > /usr/local/etc/journald-cloudwatch-logs.conf

                      rm /var/lib/ethereum/geth.ipc || true
                      rm /etc/systemd/system/geth.service || true
                      ln -s /etc/systemd/system/geth-replica.service /etc/systemd/system/geth.service
                      systemctl daemon-reload
                      systemctl enable geth-replica.service
                      systemctl start geth-replica.service

                      /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource ReplicaLaunchTemplate --configsets cs_install --region ${AWS::Region}

                      if [ -f /usr/bin/replica-hook ]
                      then
                      /usr/bin/replica-hook
                      fi
                    - ClusterId:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                      ReplicaHTTPFlag:
                          !If [HasReplicaHTTP, "--http --http.addr 0.0.0.0 --http.port 8545", ""]
                      ReplicaGraphQLFlag:
                          # Wow that's a lot of escaping!
                          !If [HasReplicaGraphQL, "--graphql --graphql.vhosts \\\\\\'*\\\\\\'", ""]
                      ReplicaWebsocketsFlag:
                          !If [HasReplicaWebsockets, "--ws --ws.addr 0.0.0.0 --ws.port 8546", ""]
                      BaseInfrastructure:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-BaseInfrastructure"
                      FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]
                      EventsTopicFlag: !If [ HasEventsTopic, !Sub "--kafka.event.topic ${EventsTopic}", ""]
                mode: "000700"
                owner: root
                group: root
              /root/start-fallback.sh:
                content:
                  "Fn::Sub":
                    - |
                      #!/bin/bash -xe

                      printf "log_group = \"${FallBackLG}\"\nstate_file = \"/var/lib/journald-cloudwatch-logs/state\"" > /usr/local/etc/journald-cloudwatch-logs.conf

                      rm /var/lib/ethereum/geth.ipc || true
                      rm /etc/systemd/system/geth.service || true
                      ln -s /etc/systemd/system/geth-fallback.service /etc/systemd/system/geth.service
                      systemctl daemon-reload
                      systemctl enable geth-fallback.service
                      systemctl start geth-fallback.service

                      /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource ReplicaLaunchTemplate --configsets cs_install --region ${AWS::Region}

                      if [ -f /usr/bin/replica-hook ]
                      then
                      /usr/bin/replica-hook
                      fi
                    - ClusterId:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                      ReplicaHTTPFlag:
                          !If [HasReplicaHTTP, "--http --http.addr 0.0.0.0 --http.port 8545", ""]
                      ReplicaGraphQLFlag:
                          # Wow that's a lot of escaping!
                          !If [HasReplicaGraphQL, "--graphql --graphql.vhosts \\\\\\'*\\\\\\'", ""]
                      ReplicaWebsocketsFlag:
                          !If [HasReplicaWebsockets, "--ws --ws.addr 0.0.0.0 --ws.port 8546", ""]
                      BaseInfrastructure:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-BaseInfrastructure"
                      FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]
                      EventsTopicFlag: !If [ HasEventsTopic, !Sub "--kafka.event.topic ${EventsTopic}", ""]
                mode: "000700"
                owner: root
                group: root
              /root/start-master-overlay.sh:
                content:
                  "Fn::Sub":
                    - |
                      #!/bin/bash -xe
                      printf "log_group = \"${MasterLG}\"
                      state_file = \"/var/lib/journald-cloudwatch-logs/state\"" > /usr/local/etc/journald-cloudwatch-logs.conf

                      rm -f /var/lib/ethereum/geth/nodekey || true
                      rm /var/lib/ethereum/geth.ipc || true
                      rm /etc/systemd/system/geth.service || true
                      ln -s /etc/systemd/system/geth-master-overlay.service /etc/systemd/system/geth.service
                      systemctl daemon-reload
                      systemctl enable geth-master-overlay.service
                      systemctl enable geth-tx.service
                      systemctl enable geth-peer-data.service

                      systemctl start geth-master-overlay.service || true &
                      sleep 5
                      systemctl start geth-tx.service
                      systemctl start geth-peer-data.service

                    - ClusterId:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                      ReplicaHTTPFlag:
                          !If [HasReplicaHTTP, "--http --http.addr 0.0.0.0 --http.port 8545", ""]
                      ReplicaGraphQLFlag:
                          # Wow that's a lot of escaping!
                          !If [HasReplicaGraphQL, "--graphql --graphql.vhosts \\\\\\'*\\\\\\'", ""]
                      ReplicaWebsocketsFlag:
                          !If [HasReplicaWebsockets, "--ws --ws.addr 0.0.0.0 --ws.port 8546", ""]
                      BaseInfrastructure:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-BaseInfrastructure"
                      FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]
                      EventsTopicFlag: !If [ HasEventsTopic, !Sub "--kafka.event.topic ${EventsTopic}", ""]
                mode: "000700"
                owner: root
                group: root
              /root/stop-all.sh:
                content:
                  "Fn::Sub":
                    - |
                      #!/bin/bash -xe

                      systemctl stop amazon-cloudwatch-agent.service || true &
                      systemctl stop journald-cloudwatch-logs || true &
                      systemctl stop geth-fallback.service || true &
                      systemctl stop geth-master.service  || true &
                      systemctl stop geth-master-overlay.service  || true &
                      systemctl stop geth-peer-data.service || true &
                      systemctl stop geth-tx.service || true &
                      systemctl stop geth-replica.service || true &
                      wait

                    - ClusterId:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                      ReplicaHTTPFlag:
                          !If [HasReplicaHTTP, "--http --http.addr 0.0.0.0 --http.port 8545", ""]
                      ReplicaGraphQLFlag:
                          # Wow that's a lot of escaping!
                          !If [HasReplicaGraphQL, "--graphql --graphql.vhosts \\\\\\'*\\\\\\'", ""]
                      ReplicaWebsocketsFlag:
                          !If [HasReplicaWebsockets, "--ws --ws.addr 0.0.0.0 --ws.port 8546", ""]
                      BaseInfrastructure:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-BaseInfrastructure"
                      FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]
                      EventsTopicFlag: !If [ HasEventsTopic, !Sub "--kafka.event.topic ${EventsTopic}", ""]
                mode: "000700"
                owner: root
                group: root
              /root/termination-detection.sh:
                content:
                  "Fn::Sub":
                    - |
                      #!/bin/bash
                      TOKEN=`curl -s -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"`
                      while sleep 5; do
                          HTTP_CODE=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" -s -w %{http_code} -o /dev/null http://169.254.169.254/latest/meta-data/spot/instance-action)
                          if [[ "$HTTP_CODE" -eq 401 ]] ; then
                              # echo "Refreshing Authentication Token"
                              TOKEN=`curl -s -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 30"`
                          elif [[ "$HTTP_CODE" -eq 200 ]] ; then
                              echo "Detected termination signal. Shutting down"
                              systemctl stop proxy || true
                              /root/stop-all.sh
                              sleep 10
                              poweroff
                          # else
                              # echo "Not Interrupted"
                          fi
                      done
                    - ClusterId:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                mode: "000700"
                owner: root
                group: root
              /root/loadtest-disk.sh:
                content:
                  "Fn::Sub":
                    - |
                      #!/bin/bash -xe
                      fio --filename=/dev/sdf --rw=read --bs=128k --iodepth=32 --ioengine=libaio --prio=7 --prioclass=3 --thinktime=2 --rate_iops=4000 --direct=1 --name=volume-initialize
                    - ClusterId:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                      ReplicaHTTPFlag:
                          !If [HasReplicaHTTP, "--http --http.addr 0.0.0.0 --http.port 8545", ""]
                      ReplicaGraphQLFlag:
                          # Wow that's a lot of escaping!
                          !If [HasReplicaGraphQL, "--graphql --graphql.vhosts \\\\\\'*\\\\\\'", ""]
                      ReplicaWebsocketsFlag:
                          !If [HasReplicaWebsockets, "--ws --ws.addr 0.0.0.0 --ws.port 8546", ""]
                      BaseInfrastructure:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-BaseInfrastructure"
                      FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]
                      EventsTopicFlag: !If [ HasEventsTopic, !Sub "--kafka.event.topic ${EventsTopic}", ""]
                mode: "000700"
                owner: root
                group: root

    Properties:
      LaunchTemplateData:
        ImageId: !If [HasReplicaImageAMI, !Ref ReplicaImageAMI, !FindInMap [RegionMap, !Ref "AWS::Region", AL2AMI]]
        InstanceType: m5.large
        TagSpecifications:
          - ResourceType: instance
            Tags:
              - Key: Name
                Value: !Sub "${KafkaTopic}-Master"
          - ResourceType: volume
            Tags:
              - Key: Name
                Value: !Sub "${KafkaTopic}-Master"
        SecurityGroupIds:
          - !Sub ${MasterNodeSecurityGroup.GroupId}
        IamInstanceProfile:
          Name: !Ref MasterNodeInstanceProfile
        KeyName: !If [HasKeyName, !Ref KeyName, !Ref 'AWS::NoValue']
        CreditSpecification: !If [SmallMaster, {CpuCredits: standard}, !Ref 'AWS::NoValue']
        BlockDeviceMappings:
        - DeviceName: "/dev/xvda"
          Ebs:
            VolumeSize: 8
            VolumeType: gp2
        - DeviceName: "/dev/sds"
          Ebs:
            VolumeSize: 8
            VolumeType: gp3
        - DeviceName: "/dev/sdf"
          Ebs:
            VolumeSize: !Ref DiskSize
            VolumeType: io1
            Iops: !GetAtt VolumeIOPS.Value
            SnapshotId: !Ref SnapshotId
        - !If [ HasSnapsdb, {DeviceName: "/dev/sdh", Ebs: {VolumeSize: !Ref SnapsdbSize, VolumeType: gp3}}, !Ref 'AWS::NoValue']
        UserData:
          "Fn::Base64":
            "Fn::Sub":
              - |
                #!/bin/bash -xe
                /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource MasterLaunchTemplate --configsets setup --region ${AWS::Region} || (sleep 30; /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource MasterLaunchTemplate --configsets setup --region ${AWS::Region} || poweroff)
                /root/service-configs.sh || (sleep 30; /root/service-configs.sh || poweroff)
                /root/configure-server.sh || (sleep 30; /root/configure-server.sh || poweroff)
                /root/start-master.sh  || (sleep 30; /root/start-master.sh  || poweroff)
                /root/loadtest-disk.sh
                export AWS_DEFAULT_REGION=${AWS::Region}
                VOLUME_ID=$(aws ec2 describe-volumes --filters Name=attachment.instance-id,Values="$(curl http://169.254.169.254/latest/meta-data/instance-id)" | jq '.Volumes[] | select(. | .Attachments[0].Device == "/dev/sdf") | .VolumeId' -cr)
                /usr/local/bin/aws ec2 modify-volume --volume-id $VOLUME_ID --volume-type gp3 --iops 3000 --throughput 125 &
              - ClusterId:
                  "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                BaseInfrastructure:
                  "Fn::ImportValue": !Sub "${InfrastructureStack}-BaseInfrastructure"
                EventsTopicFlag: !If [ HasEventsTopic, !Sub "--kafka.event.topic ${EventsTopic}", ""]
                FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]

  MasterAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      VPCZoneIdentifier:
        - "Fn::ImportValue":
            !Sub "${InfrastructureStack}-PublicA"
        - "Fn::ImportValue":
            !Sub "${InfrastructureStack}-PublicB"
        - "Fn::ImportValue":
            !Sub "${InfrastructureStack}-PublicC"
      # LaunchTemplate:
      #   LaunchTemplateId: !Ref MasterLaunchTemplate
      #   Version: !Sub ${MasterLaunchTemplate.LatestVersionNumber}
      MinSize: !Ref MasterCount
      MaxSize: 7
      HealthCheckType: EC2
      CapacityRebalance: false
      MixedInstancesPolicy:
        InstancesDistribution:
          SpotAllocationStrategy: !Ref MasterSpotAllocationStrategy
          OnDemandPercentageAboveBaseCapacity: !Ref MasterOnDemandPercentage
          SpotInstancePools: !If [ MasterSpotLowestPrice, !FindInMap [PoolSize, Size, !Ref MasterSize], !Ref "AWS::NoValue"]
        LaunchTemplate:
          LaunchTemplateSpecification:
            LaunchTemplateId: !Ref MasterLaunchTemplate
            Version: !Sub ${MasterLaunchTemplate.LatestVersionNumber}
          Overrides: !FindInMap [InstanceSizes, Master, !Ref MasterSize]
      MetricsCollection:
      - Granularity: 1Minute
        Metrics:
        - GroupInServiceInstances
      Tags:
      - Key: Name
        Value: !Sub ${AWS::StackName}-Master
        PropagateAtLaunch: 'true'

  AggregatedNotifications:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: Aggregated Notifications
  AggregatedNotificationsSubscription:
    Type: AWS::SNS::Subscription
    Condition: HasNotificationEmail
    Properties:
      Endpoint: !Ref NotificationEmail
      Protocol: email
      TopicArn: !Ref AggregatedNotifications
  UrgentNotifications:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: Urgent Notifications
  UrgentNotificationsSubscription:
    Type: AWS::SNS::Subscription
    Condition: HasUrgentWebhook
    Properties:
      Endpoint: !Ref UrgentAlarmWebhook
      Protocol: https
      TopicArn: !Ref UrgentNotifications
  WarningNotifications:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: Warning Notifications
  WarningNotificationsSubscription:
    Type: AWS::SNS::Subscription
    Condition: HasWarningWebhook
    Properties:
      Endpoint: !Ref WarningAlarmWebhook
      Protocol: https
      TopicArn: !Ref WarningNotifications
  ReplicaOverlayDiskSNS:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: Replica Overlay Disk
  ReplicaDiskOverlayAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref ReplicaOverlayDiskSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the overlay data directory > 95% full"
      ComparisonOperator: "GreaterThanThreshold"
      Metrics:
        - Id: nvme
          MetricStat:
            Metric:
              MetricName: "disk_used_percent"
              Namespace: CWAgent
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref ReplicaAutoScalingGroup
                - Name: device
                  Value : "nvme2n1"
                - Name: fstype
                  Value : "ext4"
                - Name: path
                  Value : "/var/lib/ethereum/overlay"
            Period: 60
            Stat: Maximum
          Label: NVME Overlay Volume Disk Usage
          ReturnData: false
        - Id: nvme0
          MetricStat:
            Metric:
              MetricName: "disk_used_percent"
              Namespace: CWAgent
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref ReplicaAutoScalingGroup
                - Name: device
                  Value : "nvme0n1"
                - Name: fstype
                  Value : "ext4"
                - Name: path
                  Value : "/var/lib/ethereum/overlay"
            Period: 60
            Stat: Maximum
          Label: NVME Overlay Volume Disk Usage
          ReturnData: false
        - Id: nvme4
          MetricStat:
            Metric:
              MetricName: "disk_used_percent"
              Namespace: CWAgent
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref ReplicaAutoScalingGroup
                - Name: device
                  Value : "nvme4n1"
                - Name: fstype
                  Value : "ext4"
                - Name: path
                  Value : "/var/lib/ethereum/overlay"
            Period: 60
            Stat: Maximum
          Label: NVME Overlay Volume Disk Usage
          ReturnData: false
        # TODO: Remove the other elements once we no longer have instances with the "device" present in their config
        - Id: nodevice
          MetricStat:
            Metric:
              MetricName: "disk_used_percent"
              Namespace: CWAgent
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref ReplicaAutoScalingGroup
                - Name: fstype
                  Value : "ext4"
                - Name: path
                  Value : "/var/lib/ethereum/overlay"
            Period: 60
            Stat: Maximum
          Label: NVME Overlay Volume Disk Usage
          ReturnData: false
        - Id: ebs
          MetricStat:
            Metric:
              MetricName: "disk_used_percent"
              Namespace: CWAgent
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref ReplicaAutoScalingGroup
                - Name: device
                  Value : "nvme1n1"
                - Name: fstype
                  Value : "ext4"
                - Name: path
                  Value : "/var/lib/ethereum/overlay"
            Period: 60
            Stat: Maximum
          Label: EBS Overlay Disk Usage
          ReturnData: false
        - Id: delta
          Expression: "MAX(METRICS())"
      InsufficientDataActions:
        - !Ref ReplicaOverlayDiskSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 5
      OKActions:
        - !Ref ReplicaOverlayDiskSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      Threshold: 90
      TreatMissingData: missing
  ReplicaCPUSNS:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: Master Disk
  ReplicasCPUAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref ReplicaCPUSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the replica CPU > 80%"
      ComparisonOperator: "GreaterThanThreshold"
      Dimensions:
        - Name: AutoScalingGroupName
          Value : !Ref ReplicaAutoScalingGroup
      InsufficientDataActions:
        - !Ref ReplicaCPUSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 10
      DatapointsToAlarm: 7
      MetricName: "CPUUtilization"
      Namespace: AWS/EC2
      OKActions:
        - !Ref ReplicaCPUSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Period: 60
      Statistic: Maximum
      Threshold: 80
      TreatMissingData: missing
  MasterDiskSNS:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: Master Disk
  MasterDiskAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref MasterDiskSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the master data directory > 95% full"
      ComparisonOperator: "GreaterThanThreshold"
      Metrics:
        - Id: xvdf
          MetricStat:
            Metric:
              MetricName: "disk_used_percent"
              Namespace: CWAgent
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref MasterAutoScalingGroup
                - Name: device
                  Value : "xvdf"
                - Name: fstype
                  Value : "ext4"
                - Name: path
                  Value : "/var/lib/ethereum"
            Period: 60
            Stat: Maximum
          Label: NVME Overlay Volume Disk Usage
          ReturnData: false
        - Id: nvme1n1
          MetricStat:
            Metric:
              MetricName: "disk_used_percent"
              Namespace: CWAgent
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref MasterAutoScalingGroup
                - Name: device
                  Value : "nvme1n1"
                - Name: fstype
                  Value : "ext4"
                - Name: path
                  Value : "/var/lib/ethereum"
            Period: 60
            Stat: Maximum
          Label: EBS Overlay Disk Usage
          ReturnData: false
        # TODO: Remove the other elements once we no longer have instances with the "device" present in their config
        - Id: nodevice
          MetricStat:
            Metric:
              MetricName: "disk_used_percent"
              Namespace: CWAgent
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref MasterAutoScalingGroup
                - Name: fstype
                  Value : "ext4"
                - Name: path
                  Value : "/var/lib/ethereum"
            Period: 60
            Stat: Maximum
          Label: EBS Overlay Disk Usage
          ReturnData: false
        - Id: delta
          Expression: "MAX(METRICS())"
      InsufficientDataActions:
        - !Ref MasterDiskSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 5
      OKActions:
        - !Ref MasterDiskSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      Threshold: 95
      TreatMissingData: missing
  MasterSnapsdbDiskAlarm:
    Condition: HasSnapsdb
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref MasterDiskSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the master snapsdb directory > 90% full"
      ComparisonOperator: "GreaterThanThreshold"
      Metrics:
        - Id: nodevice
          MetricStat:
            Metric:
              MetricName: "disk_used_percent"
              Namespace: CWAgent
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref MasterAutoScalingGroup
                - Name: fstype
                  Value : "ext4"
                - Name: path
                  Value : "/var/lib/ethereum/snapsdb"
            Period: 60
            Stat: Maximum
          Label: EBS Overlay Disk Usage
          ReturnData: false
        - Id: delta
          Expression: "MAX(METRICS())"
      InsufficientDataActions:
        - !Ref MasterDiskSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 5
      OKActions:
        - !Ref MasterDiskSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      Threshold: 90
      TreatMissingData: missing
  MasterDiskAlarmEmergency:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref MasterDiskSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the master data directory > 98% full"
      ComparisonOperator: "GreaterThanThreshold"
      Metrics:
        - Id: xvdf
          MetricStat:
            Metric:
              MetricName: "disk_used_percent"
              Namespace: CWAgent
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref MasterAutoScalingGroup
                - Name: device
                  Value : "xvdf"
                - Name: fstype
                  Value : "ext4"
                - Name: path
                  Value : "/var/lib/ethereum"
            Period: 60
            Stat: Maximum
          Label: NVME Overlay Volume Disk Usage
          ReturnData: false
        - Id: nvme1n1
          MetricStat:
            Metric:
              MetricName: "disk_used_percent"
              Namespace: CWAgent
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref MasterAutoScalingGroup
                - Name: device
                  Value : "nvme1n1"
                - Name: fstype
                  Value : "ext4"
                - Name: path
                  Value : "/var/lib/ethereum"
            Period: 60
            Stat: Maximum
          Label: EBS Overlay Disk Usage
          ReturnData: false
        # TODO: Remove the other elements once we no longer have instances with the "device" present in their config
        - Id: nodevice
          MetricStat:
            Metric:
              MetricName: "disk_used_percent"
              Namespace: CWAgent
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref MasterAutoScalingGroup
                - Name: fstype
                  Value : "ext4"
                - Name: path
                  Value : "/var/lib/ethereum"
            Period: 60
            Stat: Maximum
          Label: EBS Overlay Disk Usage
          ReturnData: false
        - Id: delta
          Expression: "MAX(METRICS())"
      InsufficientDataActions:
        - !Ref MasterDiskSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 5
      OKActions:
        - !Ref MasterDiskSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Threshold: 98
      TreatMissingData: missing
  MasterMemSNS:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: Master RAM
  MasterMemAlarmLong:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref MasterMemSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: !Sub "Alarms when the master RAM > ${MasterMemoryThresholdLong} for 30 minutes"
      ComparisonOperator: "GreaterThanThreshold"
      Dimensions:
        - Name: AutoScalingGroupName
          Value : !Ref MasterAutoScalingGroup
      InsufficientDataActions:
        - !Ref MasterMemSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 30
      DatapointsToAlarm: 28
      MetricName: "mem_used_percent"
      Namespace: CWAgent
      OKActions:
        - !Ref MasterMemSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Period: 60
      Statistic: Maximum
      Threshold: !Ref MasterMemoryThresholdLong
      TreatMissingData: missing
  MasterMemAlarmLow:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref MasterMemSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: !Sub "Alarms when the master RAM < ${MasterMemoryThresholdLow} for 5 minutes"
      ComparisonOperator: "LessThanThreshold"
      Dimensions:
        - Name: AutoScalingGroupName
          Value : !Ref MasterAutoScalingGroup
      InsufficientDataActions:
        - !Ref MasterMemSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 5
      MetricName: "mem_used_percent"
      Namespace: CWAgent
      OKActions:
        - !Ref MasterMemSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      Period: 60
      Statistic: Minimum
      Threshold: !Ref MasterMemoryThresholdLow
      TreatMissingData: missing
  MasterMemAlarmHigh:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref MasterMemSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: !Sub "Alarms when the master RAM > ${MasterMemoryThresholdHigh} for 5 minutes%"
      ComparisonOperator: "GreaterThanThreshold"
      Dimensions:
        - Name: AutoScalingGroupName
          Value : !Ref MasterAutoScalingGroup
      InsufficientDataActions:
        - !Ref MasterMemSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 5
      MetricName: "mem_used_percent"
      Namespace: CWAgent
      OKActions:
        - !Ref MasterMemSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      Period: 60
      Statistic: Maximum
      Threshold: !Ref MasterMemoryThresholdHigh
      TreatMissingData: missing
  MasterCPUSNS:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: Master RAM
  MasterCPUAlarmLong:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref MasterCPUSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the master CPU > 80%"
      ComparisonOperator: "GreaterThanThreshold"
      Dimensions:
        - Name: AutoScalingGroupName
          Value : !Ref MasterAutoScalingGroup
      InsufficientDataActions:
        - !Ref MasterCPUSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 30
      DatapointsToAlarm: 28
      MetricName: "CPUUtilization"
      Namespace: AWS/EC2
      OKActions:
        - !Ref MasterCPUSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      Period: 60
      Statistic: Maximum
      Threshold: 80
      TreatMissingData: missing
  MasterCPUAlarmHigh:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref MasterCPUSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the master CPU > 80%"
      ComparisonOperator: "GreaterThanThreshold"
      Dimensions:
        - Name: AutoScalingGroupName
          Value : !Ref MasterAutoScalingGroup
      InsufficientDataActions:
        - !Ref MasterCPUSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 5
      MetricName: "CPUUtilization"
      Namespace: AWS/EC2
      OKActions:
        - !Ref MasterCPUSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Period: 60
      Statistic: Maximum
      Threshold: 95
      TreatMissingData: missing
  MasterPeerCountSNS:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: Master Peer Count
  MasterPeerCountAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref MasterPeerCountSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the master PeerCount < 10"
      ComparisonOperator: "LessThanThreshold"
      Dimensions:
        - Name: clusterId
          Value : !Ref KafkaTopic
      InsufficientDataActions:
        - !Ref MasterPeerCountSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 5
      MetricName: "peerCount"
      Namespace: BlockData
      OKActions:
        - !Ref MasterPeerCountSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      Period: 60
      Statistic: Maximum
      Threshold: 10
      TreatMissingData: missing
  MasterPeerCountAlarmEmergency:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref MasterPeerCountSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the master PeerCount < 10"
      ComparisonOperator: "LessThanThreshold"
      Dimensions:
        - Name: clusterId
          Value : !Ref KafkaTopic
      InsufficientDataActions:
        - !Ref MasterPeerCountSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 5
      MetricName: "peerCount"
      Namespace: BlockData
      OKActions:
        - !Ref MasterPeerCountSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Period: 60
      Statistic: Maximum
      Threshold: 10
      TreatMissingData: missing
  MissingTrieNodeAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when missingTrie > 1000"
      ComparisonOperator: "GreaterThanThreshold"
      Dimensions:
        - Name: clusterId
          Value : !Ref KafkaTopic
      InsufficientDataActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 1
      MetricName: "trieMissing"
      Namespace: ReplicaData
      OKActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Period: 60
      Statistic: Sum
      Threshold: 1000
      TreatMissingData: notBreaching

  ReplicaCountAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: !Sub "Alarms when the number of replicas < desired capacity"
      ComparisonOperator: "LessThanThreshold"

      InsufficientDataActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 5
      DatapointsToAlarm: 2
      Metrics:
        - Id: instances
          MetricStat:
            Metric:
              MetricName: "GroupInServiceInstances"
              Namespace: AWS/AutoScaling
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref ReplicaAutoScalingGroup
            Period: 60
            Stat: Minimum
          Label: Instances
          ReturnData: false
        - Id: measured
          Expression: "(instances + 1)"
      OKActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Threshold: !Ref ReplicaTargetCapacity
      TreatMissingData: missing
  MasterCountAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: !Sub "Alarms when the number of replicas < desired capacity"
      ComparisonOperator: "LessThanThreshold"

      InsufficientDataActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 5
      DatapointsToAlarm: 2
      Metrics:
        - Id: instances
          MetricStat:
            Metric:
              MetricName: "GroupInServiceInstances"
              Namespace: AWS/AutoScaling
              Dimensions:
                - Name: AutoScalingGroupName
                  Value : !Ref MasterAutoScalingGroup
            Period: 60
            Stat: Minimum
          Label: Instances
          ReturnData: false
        - Id: measured
          Expression: "(instances + 1)"
      OKActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Threshold: !Ref MasterCount
      TreatMissingData: missing
  MasterLogMetricsFunctionLG:
    Type: AWS::Logs::LogGroup
    Properties:
      RetentionInDays: 7
      LogGroupName: !Join ["", ["/aws/lambda/", !Ref MasterLogMetricsFunction]]
  ReplicaLogMetricsFunctionLG:
    Type: AWS::Logs::LogGroup
    Properties:
      RetentionInDays: 7
      LogGroupName: !Join ["", ["/aws/lambda/", !Ref ReplicaLogMetricsFunction]]

  ReplicaMemAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: !Sub "Alarms when the replica RAM > ${ReplicaMemoryThresholdHigh} for 5 minutes%"
      ComparisonOperator: "GreaterThanThreshold"
      Dimensions:
        - Name: AutoScalingGroupName
          Value : !Ref ReplicaAutoScalingGroup
      EvaluationPeriods: 5
      MetricName: "mem_used_percent"
      Namespace: CWAgent
      OKActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      Period: 60
      Statistic: Maximum
      Threshold: !Ref ReplicaMemoryThresholdHigh
      TreatMissingData: missing

  ReplicaSwapAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: !Sub "Alarms when the replica swap > 5%"
      ComparisonOperator: "GreaterThanThreshold"
      Dimensions:
        - Name: AutoScalingGroupName
          Value : !Ref ReplicaAutoScalingGroup
      EvaluationPeriods: 1
      MetricName: "swap_used_percent"
      Namespace: CWAgent
      OKActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Period: 60
      Statistic: Maximum
      Threshold: 5
      TreatMissingData: missing

  LogMetricsRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
        Version: '2012-10-17'
  LogMetricsFunctionPolicy:
    Type: "AWS::IAM::Policy"
    Properties:
      Roles:
        - !Ref LogMetricsRole
      PolicyName: !Sub "MasterLogMetrics${AWS::StackName}"
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action:
              - "logs:CreateLogStream"
              - "logs:PutLogEvents"
            Resource: "*"
          - Effect: Allow
            Action:
              - "cloudwatch:PutMetricData"
            Resource: "*"
          - Effect: Allow
            Action:
              - "logs:CreateLogGroup"
            Resource: !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*"
  MasterLogMetricsFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Code:
        S3Bucket: !Ref S3GethBucketName
        S3Key: lambdaPackage-22.zip
      Description: "A lambda function to process Geth logs into metrics"
      Environment:
        Variables:
          CLUSTER_ID: !Sub ${KafkaTopic}
      Handler: "logMonitor.masterHandler"
      Role: !Sub ${LogMetricsRole.Arn}
      Runtime: python3.7
  MasterLogMetricsSubscription:
    Type: AWS::Logs::SubscriptionFilter
    Properties:
      DestinationArn: !Sub ${MasterLogMetricsFunction.Arn}
      FilterPattern: '{$.systemdUnit = "geth.service" || $.systemdUnit = "geth-master.service" || $.systemdUnit = "geth-peer-data.service"}'
      LogGroupName: !Ref MasterLG
  MasterLogMetricFunctionInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Sub ${MasterLogMetricsFunction.Arn}
      Action: 'lambda:InvokeFunction'
      Principal: !Sub logs.${AWS::Region}.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !Sub ${MasterLG.Arn}

  MasterBlockAgeSNS:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: Block Age
  MasterBlockAgeAlarm:
    Condition: NoRemoteRPCURL
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref MasterBlockAgeSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the block age > 120"
      ComparisonOperator: "GreaterThanThreshold"
      Dimensions:
        - Name: clusterId
          Value : !Ref KafkaTopic
      EvaluationPeriods: 3
      MetricName: "age"
      Namespace: BlockData
      OKActions:
        - !Ref MasterBlockAgeSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Period: 30
      Statistic: Maximum
      Threshold: 120
      TreatMissingData: ignore

  MasterBlockNumberSNS:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: Block Number
  MasterBlockNumberAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: NoRemoteRPCURL
    Properties:
      AlarmActions:
        - !Ref MasterBlockNumberSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the block number is missing"
      ComparisonOperator: "LessThanThreshold"
      Dimensions:
        - Name: clusterId
          Value : !Ref KafkaTopic
      EvaluationPeriods: 3
      MetricName: "number"
      Namespace: BlockData
      OKActions:
        - !Ref MasterBlockNumberSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Period: 30
      Statistic: SampleCount
      Threshold: 1
      TreatMissingData: breaching
  MasterBlockNumberComparisonAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: HasRemoteRPCURL
    Properties:
      AlarmActions:
        - !Ref MasterBlockNumberSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the block number is missing"
      ComparisonOperator: "GreaterThanThreshold"
      EvaluationPeriods: 2
      DatapointsToAlarm: 2
      Metrics:
        - Id: remote
          MetricStat:
            Metric:
              MetricName: "RemoteBlockNumber"
              Namespace: BlockData
              Dimensions:
                - Name: provider
                  Value : !Ref RemoteRPCURL
            Period: 60
            Stat: Maximum
          Label: Remote Block Number
          ReturnData: false
        - Id: cluster
          MetricStat:
            Metric:
              MetricName: "number"
              Namespace: BlockData
              Dimensions:
                - Name: clusterId
                  Value : !Ref KafkaTopic
            Period: 60
            Stat: Maximum
          Label: Cluster Block Number
          ReturnData: false
        - Id: delta
          Expression: "(remote - cluster)"
      OKActions:
        - !Ref MasterBlockNumberSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Threshold: 1
      TreatMissingData: breaching
  MasterMinBlockNumberComparisonAlarm:
    Type: AWS::CloudWatch::Alarm
    Condition: HasRemoteRPCURL
    Properties:
      AlarmActions:
        - !Ref MasterBlockNumberSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the furthest behind master is too far behind other nodes"
      ComparisonOperator: "GreaterThanThreshold"
      EvaluationPeriods: 2
      DatapointsToAlarm: 2
      Metrics:
        - Id: remote
          MetricStat:
            Metric:
              MetricName: "RemoteBlockNumber"
              Namespace: BlockData
              Dimensions:
                - Name: provider
                  Value : !Ref RemoteRPCURL
            Period: 60
            Stat: Maximum
          Label: Remote Block Number
          ReturnData: false
        - Id: cluster
          MetricStat:
            Metric:
              MetricName: "number"
              Namespace: BlockData
              Dimensions:
                - Name: clusterId
                  Value : !Ref KafkaTopic
            Period: 60
            Stat: Minimum
          Label: Cluster Block Number
          ReturnData: false
        - Id: delta
          Expression: "(remote - cluster)"
      OKActions:
        - !Ref MasterBlockNumberSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Threshold: 20 # Leave some wiggle room because it'll compare the smallest value in a 1 minute period to the biggest value of the other metric, and we can expect a decent bit of fluctuation even when everything is cool
      TreatMissingData: breaching
  FlumeBlockNumberComparisonAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref MasterBlockNumberSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the block number is missing"
      ComparisonOperator: "GreaterThanThreshold"
      EvaluationPeriods: 2
      Metrics:
        - Id: flume
          MetricStat:
            Metric:
              MetricName: "block"
              Namespace: FlumeData
              Dimensions:
                - Name: clusterId
                  Value : !Ref NetworkId
            Period: 60
            Stat: Minimum
          Label: Remote Block Number
          ReturnData: false
        - Id: cluster
          MetricStat:
            Metric:
              MetricName: "number"
              Namespace: BlockData
              Dimensions:
                - Name: clusterId
                  Value : !Ref KafkaTopic
            Period: 60
            Stat: Minimum
          Label: Cluster Block Number
          ReturnData: false
        - Id: delta
          Expression: "(cluster - flume)"
      OKActions:
        - !Ref MasterBlockNumberSNS
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Threshold: 10 # If the lowest value from Flume is 10 behind the lowest value from a master in a period of time, a flume server is not keeping up
                    # This still isn't perfect, because the metric comes from log messages that won't happen if Flume isn't processing data
      TreatMissingData: breaching
  SnapshotAgeWarning:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      AlarmDescription: "Alarms when the latest snapshot exceeds a certain age"
      ComparisonOperator: "GreaterThanThreshold"
      EvaluationPeriods: 2
      Metrics:
        - Id: current
          MetricStat:
            Metric:
              MetricName: "UnixTimestamp"
              Namespace: Consul
            Period: 60
            Stat: Maximum
          Label: Unix Timestamp
          ReturnData: false
        - Id: delta
          Expression: !Sub "(current - ${SnapshotTimestamp})"
      OKActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasWarningWebhook, !Ref WarningNotifications, !Ref 'AWS::NoValue' ]
      Threshold: 43200 # 12 hours in seconds
      TreatMissingData: breaching

  ReplicaLG:
    Type: AWS::Logs::LogGroup
    Properties:
      RetentionInDays: 7
      LogGroupName:
        "Fn::Sub":
          - "/${ClusterId}/${AWS::StackName}/replica"
          - ClusterId:
              "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
  ReplicaNodeSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Allow internal SSH access and VPC access to RPC
      VpcId:
        "Fn::ImportValue": !Sub "${InfrastructureStack}-VpcId"
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: '22'
        ToPort: '22'
        CidrIp: !Join ["", ["Fn::ImportValue": !Sub "${InfrastructureStack}-VpcBaseIp", ".0.0/14"]]
      - IpProtocol: tcp
        FromPort: '8545'
        ToPort: '8545'
        CidrIp: !Join ["", ["Fn::ImportValue": !Sub "${InfrastructureStack}-VpcBaseIp", ".0.0/16"]]
      - IpProtocol: tcp
        # Consul sidecar proxy ports
        FromPort: '21000'
        ToPort: '21016'
        CidrIp: !Ref ConsulCIDRBlock
      - IpProtocol: tcp
        # Consul agent gossip
        FromPort: '8301'
        ToPort: '8301'
        CidrIp: !Ref ConsulCIDRBlock
      - IpProtocol: udp
        # Consul agent gossip
        FromPort: '8301'
        ToPort: '8301'
        CidrIp: !Ref ConsulCIDRBlock
      - IpProtocol: tcp
        FromPort: '8546'
        ToPort: '8546'
        CidrIp: !Join ["", ["Fn::ImportValue": !Sub "${InfrastructureStack}-VpcBaseIp", ".0.0/16"]]
      - IpProtocol: tcp
        FromPort: '8547'
        ToPort: '8547'
        CidrIp: !Join ["", ["Fn::ImportValue": !Sub "${InfrastructureStack}-VpcBaseIp", ".0.0/16"]]
  ReplicaNodeRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - ec2.amazonaws.com
            - autoscaling.amazonaws.com
        Version: '2012-10-17'
  ReplicaNodePolicy:
    Type: "AWS::IAM::Policy"
    Properties:
      Roles:
        - !Ref ReplicaNodeRole
      PolicyName: !Sub "ReplicaNode${AWS::StackName}"
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action: "ec2:DescribeInstances"
            Resource: "*"
          - Action:
              - logs:CreateLogStream
              - logs:CreateLogGroup
              - logs:PutLogEvents
            Effect: Allow
            Resource: "*"
            Sid: Stmt3
          - Action:
              - s3:GetObject
              - s3:ListBucket
              - s3:GetBucketPolicy
              - s3:GetObjectTagging
              - s3:GetBucketLocation
            Resource: !Sub arn:aws:s3:::${S3GethBucketName}/*
            Effect: Allow
          - Action:
              - s3:GetObject
              - s3:PutObject
              - s3:ListBucket
              - s3:GetBucketPolicy
              - s3:GetObjectTagging
              - s3:GetBucketLocation
            Resource:
              - {"Fn::Sub": ["arn:aws:s3:::${Bucket}/*", {"Bucket": !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]}]}
              - {"Fn::Sub": ["arn:aws:s3:::${Bucket}", {"Bucket": !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]}]}
            Effect: Allow
          - Action:
              - cloudwatch:PutMetricData
              - ec2:DescribeTags
              - logs:PutLogEvents
              - logs:DescribeLogStreams
              - logs:DescribeLogGroups
              - logs:CreateLogStream
              - logs:CreateLogGroup
            Resource: "*"
            Effect: Allow
          - Action:
              - ssm:GetParameter
            Resource: !Sub "arn:aws:ssm:*:*:parameter/${MetricsConfigParameter}"
            Effect: Allow
          - Action:
              - ec2:ModifyVolume
              - ec2:DescribeVolumes
            Effect: Allow
            Resource: "*"
  ReplicaNodeInstanceProfile:
    Type: "AWS::IAM::InstanceProfile"
    Properties:
      Path: /
      Roles:
      - !Ref ReplicaNodeRole
    DependsOn: ReplicaNodeRole

  ReplicaLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Metadata:
      AWS::CloudFormation::Init:
          configSets:
            cs_install:
              - install_and_enable_cfn_hup
              - create_consul_group_user_dir
              - install_consul
              - install_consul_template
              - consul_bootstrap
          install_and_enable_cfn_hup:
            files:
              /etc/cfn/cfn-hup.conf:
                content: !Join
                  - ""
                  - - |
                      [main]
                    - stack=
                    - !Ref "AWS::StackId"
                    - |+

                    - region=
                    - !Ref "AWS::Region"
                    - |+
                mode: "000400"
                owner: root
                group: root
              /etc/cfn/hooks.d/cfn-auto-reloader.conf:
                content: !Join
                  - ""
                  - - |
                      [cfn-auto-reloader-hook]
                    - |
                      triggers=post.update
                    - >
                      path=Resources.ReplicaLaunchTemplate.Metadata.AWS::CloudFormation::Init
                    - "action=/usr/local/bin/cfn-init -v "
                    - "         --stack "
                    - !Ref "AWS::StackName"
                    - "         --resource ReplicaLaunchTemplate "
                    - "         --configsets cs_install "
                    - "         --region "
                    - !Ref "AWS::Region"
                    - |+

                    - |
                      runas=root
              /lib/systemd/system/cfn-hup.service:
                content: !Join
                  - ""
                  - - |
                      [Unit]
                    - |+
                      Description=cfn-hup daemon

                    - |
                      [Service]
                    - |
                      Type=simple
                    - |
                      ExecStart=/usr/local/bin/cfn-hup
                    - |+
                      Restart=always
                    - |
                      [Install]
                    - WantedBy=multi-user.target
            commands:
              01enable_cfn_hup:
                command: systemctl enable cfn-hup.service
              02start_cfn_hup:
                command: systemctl start cfn-hup.service
          create_consul_group_user_dir:
            users:
              consul:
                homeDir: /srv/consul
            commands:
              01_create_data_dir:
                command: mkdir -p /opt/consul/data
          install_consul:
            sources:
              /usr/bin/: https://releases.hashicorp.com/consul/1.9.1/consul_1.9.1_linux_amd64.zip
          install_consul_template:
            sources:
              /usr/bin/: https://releases.hashicorp.com/consul-template/0.24.0/consul-template_0.24.0_linux_amd64.zip
          consul_bootstrap:
            files:
              /opt/consul/config/client.json:
                content:
                  "Fn::Sub":
                    - |
                      {
                        "advertise_addr": "PrivateIpAddress",
                        "bind_addr": "PrivateIpAddress",
                        "node_name": "InstanceId",
                        "datacenter": "${ConsulAccountName}--${AWS::Region}",
                        "server": false,
                        "ui" : false,
                        "leave_on_terminate" : true,
                        "skip_leave_on_interrupt" : false,
                        "disable_update_check": true,
                        "log_level": "warn",
                        "enable_local_script_checks": true,
                        "data_dir": "/opt/consul/data",
                        "client_addr": "0.0.0.0",
                        "primary_datacenter": "${PrimaryRegion}",
                        "retry_join": ["provider=aws region=${AWS::Region} tag_key=${ConsulEc2RetryTagKey} tag_value=${ConsulEc2RetryTagValue}"],
                        "addresses": {
                          "http": "0.0.0.0"
                        },
                        "ports": { "grpc": 8502 },
                        "connect": {
                          "enabled": true
                        }
                      }
                    - PrimaryRegion: !If [ SpecifiesConsulRegion, !Ref ConsulPrimaryRegion, !Sub "${AWS::Region}"]
                mode: 000644
              /etc/systemd/system/consul.service:
                content: !Join
                  - ""
                  - - |
                      [Unit]
                    - |
                      Description="HashiCorp Consul - A service mesh solution"
                    - |
                      Documentation=https://www.consul.io/
                    - |
                      Requires=network-online.target
                    - |
                      After=network-online.target
                    - |
                      ConditionFileNotEmpty=/opt/consul/config/client.json
                    - |+
                      [Service]
                    - |
                      Type=notify
                    - |
                      User=consul
                    - |
                      Group=consul
                    - |
                      ExecStart=/usr/bin/consul agent -config-dir /opt/consul/config -data-dir /opt/consul/data
                    - |
                      ExecReload=/usr/bin/consul reload
                    - |
                      KillMode=process
                    - |
                      Restart=on-failure
                    - |
                      TimeoutSec=300s
                    - |
                      LimitNOFILE=65536
                    - |+
                      [Install]
                    - WantedBy=multi-user.target
            commands:
              00_fill_consul_config_ip:
                command: myip=`echo $(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)` && sed -i "s/PrivateIpAddress/${myip}/g" /opt/consul/config/client.json
              01_fill_consul_config_instance_id:
                command: myid=`echo $(curl -s http://169.254.169.254/latest/meta-data/instance-id)` && sed -i "s/InstanceId/${myid}/g" /opt/consul/config/client.json
              02_change_ownership:
                command: chown -R consul:consul /opt/consul
              03_reload_systemd:
                command: systemctl daemon-reload
              04_enable_consul:
                command: systemctl enable consul
              05_start_consul:
                command: systemctl start consul
              06_fetch_envoy:
                command: !Sub aws s3 cp s3://${S3GethBucketName}/envoy/v1.16.0/envoy-amd64 /usr/bin/envoy
              07_install_envoy:
                command: chmod +x /usr/bin/envoy
    Properties:
      LaunchTemplateData:
        ImageId: !If [HasReplicaImageAMI, !Ref ReplicaImageAMI, !FindInMap [RegionMap, !Ref "AWS::Region", AL2AMI]]
        InstanceType: m5ad.large
        TagSpecifications:
          - ResourceType: instance
            Tags:
              - Key: Name
                Value: !Sub "${KafkaTopic}-Replica"
              - Key: VOLUME_MGMT_GROUP
                Value: !Sub ${AWS::StackName}
          - ResourceType: volume
            Tags:
              - Key: Name
                Value: !Sub "${KafkaTopic}-Replica"
        SecurityGroupIds:
          - !Sub ${ReplicaNodeSecurityGroup.GroupId}
          - !If [HasExtraSecurityGroup, !Ref ReplicaExtraSecurityGroup, !Ref 'AWS::NoValue']
        IamInstanceProfile:
          Name: !Ref ReplicaNodeInstanceProfile
        KeyName: !If [HasKeyName, !Ref KeyName, !Ref 'AWS::NoValue']
        CreditSpecification: !If [SmallReplica, {CpuCredits: standard}, !Ref 'AWS::NoValue']
        BlockDeviceMappings:
        - DeviceName: "/dev/xvda"
          Ebs:
            VolumeSize: 8
            VolumeType: gp3
        - DeviceName: "/dev/sds"
          Ebs:
            VolumeSize: 8
            VolumeType: gp3
        - DeviceName: "/dev/sdf"
          Ebs:
            VolumeSize: !If [ReplicaHDD, !GetAtt HDDSize.Value, !Ref DiskSize]
            VolumeType: !If [ReplicaHDD, !Ref ReplicaDiskType, "io1"]
            # Iops: !If [ReplicaHDD, !Ref 'AWS::NoValue', 3000 ]
            # Throughput: !If [ReplicaHDD, !Ref 'AWS::NoValue', 125 ]
            # VolumeType: !If [ReplicaHDD, !Ref ReplicaDiskType, "gp3"]
            Iops: !If [ReplicaHDD, !Ref 'AWS::NoValue', !GetAtt VolumeIOPS.Value ]
            # Throughput: !If [ReplicaHDD, !Ref 'AWS::NoValue', 1000 ]
            SnapshotId: !Ref SnapshotId
        - !If [ SmallReplica, {DeviceName: "/dev/sdg", Ebs: {VolumeSize: 25, VolumeType: gp3, Iops: 3000, Throughput: 125}}, {DeviceName: "/dev/sdg", Ebs: {VolumeSize: 150, VolumeType: gp3, Iops: 3000, Throughput: 125}}]
        UserData:
          "Fn::Base64":
            "Fn::Sub":
              - |
                #!/bin/bash -xe
                /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource MasterLaunchTemplate --configsets setup --region ${AWS::Region} || (sleep 30; /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource MasterLaunchTemplate --configsets setup --region ${AWS::Region} || poweroff)
                /root/service-configs.sh || (sleep 30; /root/service-configs.sh || poweroff)
                /root/configure-server.sh || (sleep 30; /root/configure-server.sh || poweroff)
                /root/start-replica.sh || (sleep 30; /root/start-replica.sh || poweroff)
                /root/loadtest-disk.sh || poweroff
                export AWS_DEFAULT_REGION=${AWS::Region}
                VOLUME_ID=$(aws ec2 describe-volumes --filters Name=attachment.instance-id,Values="$(curl http://169.254.169.254/latest/meta-data/instance-id)" | jq '.Volumes[] | select(. | .Attachments[0].Device == "/dev/sdf") | .VolumeId' -cr)
                /usr/local/bin/aws ec2 modify-volume --volume-id $VOLUME_ID --volume-type gp3 --iops 3000 --throughput 125 &
              - ClusterId:
                  "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                ReplicaHTTPFlag:
                    !If [HasReplicaHTTP, "--http --http.addr 0.0.0.0 --http.port 8545", ""]
                ReplicaGraphQLFlag:
                    # Wow that's a lot of escaping!
                    !If [HasReplicaGraphQL, "--graphql --graphql.vhosts \\\\\\'*\\\\\\'", ""]
                ReplicaWebsocketsFlag:
                    !If [HasReplicaWebsockets, "--ws --ws.addr 0.0.0.0 --ws.port 8546", ""]
                BaseInfrastructure:
                  "Fn::ImportValue": !Sub "${InfrastructureStack}-BaseInfrastructure"
                FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]

  ReplicaAutoScalingPolicy:
    Type: AWS::AutoScaling::ScalingPolicy
    Properties:
      # AdjustmentType: String
      AutoScalingGroupName: !Ref ReplicaAutoScalingGroup
      # Cooldown: 900
      EstimatedInstanceWarmup: 600
      # MetricAggregationType: String
      # MinAdjustmentMagnitude: Integer
      PolicyType: TargetTrackingScaling
      # ScalingAdjustment: Integer
      # StepAdjustments:
      #   - StepAdjustment
      TargetTrackingConfiguration:
        PredefinedMetricSpecification:
          PredefinedMetricType: ASGAverageCPUUtilization
        TargetValue: !Ref ReplicaCPUScalingTargetValue

  ReplicaAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      VPCZoneIdentifier:
        - "Fn::ImportValue":
            !Sub "${InfrastructureStack}-PublicA"
        - "Fn::ImportValue":
            !Sub "${InfrastructureStack}-PublicB"
        - "Fn::ImportValue":
            !Sub "${InfrastructureStack}-PublicC"
      MixedInstancesPolicy:
        InstancesDistribution:
          OnDemandPercentageAboveBaseCapacity: !Ref ReplicaOnDemandPercentage
          SpotAllocationStrategy: !Ref ReplicaSpotAllocationStrategy
          SpotInstancePools: !If [ ReplicaSpotLowestPrice, !FindInMap [PoolSize, Size, !Ref ReplicaSize], !Ref "AWS::NoValue"]
        LaunchTemplate:
          LaunchTemplateSpecification:
            LaunchTemplateId: !Ref ReplicaLaunchTemplate
            Version: !Sub ${ReplicaLaunchTemplate.LatestVersionNumber}
          Overrides: !FindInMap [InstanceSizes, Replica, !Ref ReplicaSize]
      MinSize: !Ref ReplicaTargetCapacity
      MaxSize: !Ref ReplicaMaxCapacity
      HealthCheckType: EC2
      CapacityRebalance: false
      TargetGroupARNs: !If [NoTG, !Ref "AWS::NoValue", !Split [ ",", !If [HasATG, !Ref AlternateTargetGroup, {"Fn::ImportValue": !Sub "${InfrastructureStack}-ALBGroupList"}]]]
      MetricsCollection:
      - Granularity: 1Minute
        Metrics:
        - GroupInServiceInstances
      Tags:
      - Key: Name
        Value: !Sub ${AWS::StackName}-Replica
        PropagateAtLaunch: 'true'

  ReplicaLogMetricsFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Code:
        S3Bucket: !Ref S3GethBucketName
        S3Key: lambdaPackage-22.zip
      Description: "A lambda function to process Geth logs into metrics"
      Environment:
        Variables:
          CLUSTER_ID: !Sub ${KafkaTopic}
      Handler: "logMonitor.replicaHandler"
      Role: !Sub ${LogMetricsRole.Arn}
      Runtime: python3.7
  ReplicaLogMetricsSubscription:
    Type: AWS::Logs::SubscriptionFilter
    Properties:
      DestinationArn: !Sub ${ReplicaLogMetricsFunction.Arn}
      FilterPattern: '{$.systemdUnit = "geth.service" || $.systemdUnit="proxy.service" || $.systemdUnit="geth-replica.service"}'
      LogGroupName: !Ref ReplicaLG
  ReplicaLogMetricFunctionInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Sub ${ReplicaLogMetricsFunction.Arn}
      Action: 'lambda:InvokeFunction'
      Principal: !Sub logs.${AWS::Region}.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !Sub ${ReplicaLG.Arn}

  SnapshotterLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Metadata:
      AWS::CloudFormation::Init:
          configSets:
            s_scripting:
              - validate_block_catchup
          validate_block_catchup:
            files:
              /tmp/validate_block_catchup.sh:
                content:
                  "Fn::Sub":
                    - |
                      #!/bin/bash -xe

                      set -e
                      set -x

                      mkdir -p /tmp/overlay
                      chown -R geth /tmp/overlay

                      # make sure that geth is able to start as a full node and catch up with peers
                      sudo -Eu geth /usr/bin/geth ${MasterExtraFlags} --datadir.overlay=/tmp/overlay --rpc.allow-unprotected-txs --snapshot=false --syncmode=full --txlookuplimit=0 --cache=$allocatesafe --light.maxpeers 0 --maxpeers 25 --http --http.addr 0.0.0.0 --http.port 8545 --datadir=/var/lib/ethereum ${FreezerFlags} &
                      pid=$!
                      test_pass=0
                      while [ $test_pass -le 1 ]
                      do
                        if curl -X POST --insecure -H "Content-Type: application/json" --data '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":64}' localhost:8545
                        then
                          test_pass=$(( $test_pass + 1 ))
                        else
                          test_pass=0
                        fi
                        sleep 10
                      done
                      START_BLOCK=$(curl -X POST --insecure -H "Content-Type: application/json" --data '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":64}' localhost:8545 | jq -r '.result')
                      END_BLOCK=$(curl -X POST --insecure -H "Content-Type: application/json" --data '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":64}' localhost:8545 | jq -r '.result')
                      while [[ "$((END_BLOCK - START_BLOCK))" -le "10" ]]; do
                        END_BLOCK=$(curl -X POST --insecure -H "Content-Type: application/json" --data '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":64}' localhost:8545 | jq -r '.result')
                        sleep 10
                      done

                      kill -HUP $pid
                    - ClusterId:
                        "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                      FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]
                mode: "000700"
                owner: root
                group: root

    Properties:
      LaunchTemplateData:
        ImageId: !If [HasReplicaImageAMI, !Ref ReplicaImageAMI, !FindInMap [RegionMap, !Ref "AWS::Region", AL2AMI]]
        InstanceType: m5a.large
        TagSpecifications:
          - ResourceType: instance
            Tags:
              - Key: Name
                Value: !Sub "${KafkaTopic}-Snapshotter"
          - ResourceType: volume
            Tags:
              - Key: Name
                Value: !Sub "${KafkaTopic}-Snapshotter"
        SecurityGroupIds:
          - !Sub ${ReplicaNodeSecurityGroup.GroupId}
        IamInstanceProfile:
          Name: !Ref SnapshotterNodeInstanceProfile
        KeyName: !If [HasKeyName, !Ref KeyName, !Ref 'AWS::NoValue']
        # CreditSpecification:
        #   CpuCredits: standard
        InstanceInitiatedShutdownBehavior: terminate
        BlockDeviceMappings:
        - DeviceName: "/dev/xvda"
          Ebs:
            VolumeSize: 8
            VolumeType: gp2
        - DeviceName: "/dev/sds"
          Ebs:
            VolumeSize: 8
            VolumeType: gp3
        # - DeviceName: "/dev/sdf"
        #   Ebs:
        #     VolumeSize: !Ref DiskSize
        #     VolumeType: gp3
        #     Iops: 3000
        #     Throughput: 125
        #     SnapshotId: !Ref SnapshotId
        UserData:
          "Fn::Base64":
            "Fn::Sub":
              - |
                #!/bin/bash -xe
                if [ "$(arch)" == "x86_64" ]
                then
                  ARCH="amd64"
                elif [ "$(arch)" == "aarch64" ]
                then
                  ARCH="arm64"
                fi
                sleep 21600 && if [ "${S3BackupLogsBucket}" != "" ] ; then cat /var/log/cloud-init-output.log | aws s3 cp - s3://${S3BackupLogsBucket}/ethercattle/${NetworkId}/failure-$(date '+%Y%m%d-%H%M%S').log || true; fi && poweroff &

                totalm=$(free -m | awk '/^Mem:/{print $2}') ; echo $totalm
                export allocatesafe=$((totalm * 75 / 100))

                sysctl -p || true
                GETH_BIN="geth-linux-$ARCH"
                aws s3 cp s3://${S3GethBucketName}/${ECGethVersion}/$GETH_BIN /usr/bin/geth
                chmod +x /usr/bin/geth

                export AWS_DEFAULT_REGION=${AWS::Region}
                export AWS_REGION=${AWS::Region}

                AVAILABILITY_ZONE=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)
                INSTANCE_ID=$(curl -s http://169.254.169.254/2012-01-12/meta-data/instance-id)

                VOLUME_ID=$(aws ec2 describe-volumes --filters Name=tag:SnapshotVolume,Values=${AWS::StackName}-${AWS::Region}-${PrunerGeneration} Name=status,Values=available Name=availability-zone,Values=$AVAILABILITY_ZONE | jq -r .Volumes[0].VolumeId)

                if [ "$VOLUME_ID" == "null"  ]
                then
                VOLUME_CREATED="yes"
                  VOLUME_ID=$(aws ec2 create-volume --snapshot-id ${SnapshotId} --availability-zone $AVAILABILITY_ZONE --size ${DiskSize} --volume-type gp3 --tag-specifications '{"ResourceType": "volume", "Tags": [{"Key": "SnapshotVolume","Value": "${AWS::StackName}-${AWS::Region}-${PrunerGeneration}"}, {"Key": "Name", "Value": "${AWS::StackName}-SnapshotVolume"}]}' | jq -r .VolumeId)
                  while [ "$(aws ec2 describe-volumes --volume-id $VOLUME_ID | jq -r .Volumes[0].State)" != "available" ]
                  do
                    sleep 3
                  done
                fi
                aws ec2 attach-volume --device=/dev/sdf --instance-id=$INSTANCE_ID --volume-id=$VOLUME_ID

                while [ "$(aws ec2 describe-volumes --volume-id $VOLUME_ID | jq -r .Volumes[0].State)" != "in-use" ]
                do
                  sleep 3
                done

                dd if=/dev/sdf of=/dev/null &

                mkdir -p /var/lib/ethereum
                mount -o barrier=0,data=writeback /dev/sdf /var/lib/ethereum
                resize2fs /dev/sdf
                useradd -r geth
                chown -R geth /var/lib/ethereum

                yum install -y jq

                export AWS_DEFAULT_REGION=${AWS::Region}
                export AWS_REGION=${AWS::Region}
                printf "ReplicaClusterVersion=${ReplicaClusterVersion}\nKafkaHostname=${KafkaBrokerURL}\nKafkaTopic=${KafkaTopic}\nNetworkId=${NetworkId}\ninfraName=${InfrastructureStack}\nAWS_REGION=${AWS::Region}\n" > /etc/systemd/system/ethcattle-vars

                if [ -f /usr/bin/replica-snapshot-hook ]
                then
                  /usr/bin/replica-snapshot-hook
                fi

                sysctl -w fs.file-max=12000500
                sysctl -w fs.nr_open=20000500
                ulimit -n 20000000

                SEP=$(echo "${KafkaBrokerURL}" | grep -q "?" && echo "&" || echo "?")
                SEP_ESCAPED=$(echo "${KafkaBrokerURL}" | grep -q "?" && echo "\\&" || echo "?")
                KAFKA_ESCAPED_URL="$(printf "${KafkaBrokerURL}"| sed 's^&^\\&^g')"
                /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource SnapshotterLaunchTemplate --configsets s_scripting --region ${AWS::Region} --role ${SnapshotterNodeRole}

                # sudo -Eu geth /usr/bin/geth ${MasterExtraFlags} --rpc.allow-unprotected-txs --snapshot=false --syncmode=full --txlookuplimit=0 --cache=$allocatesafe --light.maxpeers 0 --maxpeers 0 --http --http.addr 0.0.0.0 --http.port 8545 --datadir=/var/lib/ethereum ${FreezerFlags} &
                # pid=$!
                # test_pass=0
                # while [ $test_pass -le 1 ]
                # do
                #   if curl -X POST --insecure -H "Content-Type: application/json" --data '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":64}' localhost:8545
                #   then
                #     test_pass=$(( $test_pass + 1 ))
                #   else
                #     test_pass=0
                #   fi
                #   sleep 30
                # done
                # START_BLOCK=$(curl -X POST --insecure -H "Content-Type: application/json" --data '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":64}' localhost:8545 | jq -r '.result')
                # sudo pkill -f geth
                # wait $pid
                # sleep 10


                # Update from kafka

                sudo -Eu geth /usr/bin/geth replica --snapshot=false --cache=$allocatesafe ${ReplicaExtraFlags} ${FreezerFlags} --kafka.broker=$KAFKA_ESCAPED_URL""$SEP""avoid_leader=1 --datadir=/var/lib/ethereum --kafka.topic=${KafkaTopicString} --replica.syncshutdown
                if ! sudo -Eu geth /usr/bin/geth verifystatetrie ${FreezerFlags} --datadir=/var/lib/ethereum ${SnapshotValidationThreshold}
                then
                  if [ "${AggregatedNotifications}" != "" ]
                  then
                    aws sns publish --topic-arn=${AggregatedNotifications} --subject="${AWS::StackName} - Bad State Trie" --message="State trie verification failed while taking snapshot for cluster '${KafkaTopic}'. No snapshot will be taken. This is probably unrecoverable, and you will need to deploy a new cluster from your last good snapshot (probably '${SnapshotId}')"
                  fi
                  if [ "${AlarmSNSTopic}" != "" ]
                  then
                    aws sns publish --topic-arn=${AlarmSNSTopic} --subject="${AWS::StackName} - Bad State Trie" --message="State trie verification failed while taking snapshot for cluster '${KafkaTopic}'. No snapshot will be taken. This is probably unrecoverable, and you will need to deploy a new cluster from your last good snapshot (probably '${SnapshotId}')"
                  fi
                  if [ "${S3BackupLogsBucket}" != "" ]
                  then
                    cat /var/log/cloud-init-output.log | aws s3 cp - s3://${S3BackupLogsBucket}/ethercattle/${NetworkId}/failure-$(date '+%Y%m%d-%H%M%S').log || true
                  fi
                  poweroff
                fi

                #PLACEHOLDER

                # If this process doesn't commplete in 6 hours, something has gone
                # wrong and the next snapshotter is in progress, so shutdown
                sleep 21600 && poweroff &

                VOLUME_ID=$(aws ec2 describe-volumes --filters Name=attachment.instance-id,Values="$(curl http://169.254.169.254/latest/meta-data/instance-id)" | jq '.Volumes[] | select(. | .Attachments[0].Device == "/dev/sdf") | .VolumeId' -cr)

                if [ "$VOLUME_ID" == "" ]
                then
                  if [ "${AggregatedNotifications}" != "" ]
                  then
                    aws sns publish --topic-arn=${AggregatedNotifications} --subject="${AWS::StackName} - Volume Identification Failed" --message="The snapshotting process for ${KafkaTopic} failed to identify the attached volume. Could not take a snapshot."
                  fi
                  if [ "${AlarmSNSTopic}" != "" ]
                  then
                    aws sns publish --topic-arn=${AlarmSNSTopic} --subject="${AWS::StackName} - Volume Identification Failed" --message="The snapshotting process for ${KafkaTopic} failed to identify the attached volume. Could not take a snapshot."
                  fi
                  if [ "${S3BackupLogsBucket}" != "" ]
                  then
                    cat /var/log/cloud-init-output.log | aws s3 cp - s3://${S3BackupLogsBucket}/ethercattle/${NetworkId}/failure-$(date '+%Y%m%d-%H%M%S').log || true
                  fi
                  poweroff
                fi

                SNAPSHOT_ID=`aws ec2 create-snapshot --volume-id $VOLUME_ID --tag-specification="ResourceType=snapshot,Tags=[{Key=cluster,Value=${KafkaTopic}},{Key=Name,Value=${KafkaTopic}-chaindata-$(date -Isecond -u)}]" | jq '.SnapshotId' -cr`

                if [ "$SNAPSHOT_ID" == "null" ]; then
                  if [ "${AggregatedNotifications}" != "" ]
                  then
                    aws sns publish --topic-arn=${AggregatedNotifications} --subject="${AWS::StackName} - Snapshotting Volume Failed" --message="The snapshotting process for ${KafkaTopic} failed to take a snapshot."
                  fi
                  if [ "${AlarmSNSTopic}" != "" ]
                  then
                    aws sns publish --topic-arn=${AlarmSNSTopic} --subject="${AWS::StackName} - Snapshotting Volume Failed" --message="The snapshotting process for ${KafkaTopic} failed to take a snapshot."
                  fi
                  if [ "${S3BackupLogsBucket}" != "" ]
                  then
                    cat /var/log/cloud-init-output.log | aws s3 cp - s3://${S3BackupLogsBucket}/ethercattle/${NetworkId}/failure-$(date '+%Y%m%d-%H%M%S').log || true
                  fi
                  poweroff
                fi
                echo "Waiting for snapshot to complete"
                while [ `aws ec2 describe-snapshots --filters=Name=snapshot-id,Values=$SNAPSHOT_ID | jq '.Snapshots[0].State' -cr` != "completed" ];
                do
                    sleep 10
                done


                # After the snapshot completes, sync from the network. This is an extra integrity
                # check on the snapshot, as it requires it to be able to validate blocks.
                # We don't want this to end up in our final snapshot, but if we can't sync from
                # the network we don't want to present this snapshot to the cluster.
                if ! /tmp/validate_block_catchup.sh
                then
                  if [ "${AggregatedNotifications}" != "" ]
                  then
                    aws sns publish --topic-arn=${AggregatedNotifications} --subject="${AWS::StackName} - Snapshotting Volume Failed" --message="The snapshotting process for ${KafkaTopic} took a snapshot, but Geth could not sync. Not updating CloudFormation"
                  fi
                  if [ "${AlarmSNSTopic}" != "" ]
                  then
                    aws sns publish --topic-arn=${AlarmSNSTopic} --subject="${AWS::StackName} - Snapshotting Volume Failed" --message="The snapshotting process for ${KafkaTopic} took a snapshot, but Geth could not sync. Not updating CloudFormation"
                  fi
                  if [ "${S3BackupLogsBucket}" != "" ]
                  then
                    cat /var/log/cloud-init-output.log | aws s3 cp - s3://${S3BackupLogsBucket}/ethercattle/${NetworkId}/failure-$(date '+%Y%m%d-%H%M%S').log || true
                  fi
                  poweroff
                fi

                # CFN will set any parameters we don't provide back to their default values,
                # so get all of the parameters, update SnapshotID, and update the stack with
                # the new parameters.
                PARAMETERS=$(aws cloudformation describe-stacks --stack-name ${AWS::StackName} | jq '.Stacks[0].Parameters | map(if .ParameterKey == "SnapshotId" then .ParameterValue="'$SNAPSHOT_ID'" else . end) | map(if .ParameterKey == "SnapshotTimestamp" then .ParameterValue="'$(date +%s)'" else . end) | map(if .ParameterKey == "SnapshotOffset" then .ParameterValue="" else . end)' -c)

                # Resize Disks
                ORIGINAL_DISK_SIZE=`aws ec2 describe-volumes --volume-ids "$VOLUME_ID" | jq '.Volumes[] | .Size' -cr`
                NEW_DISK_SIZE=$((ORIGINAL_DISK_SIZE*115/100))

                ## manually grepping for sdf (mount directory above) because we don't really need to worry about the others.
                df -H | grep -vE '^Filesystem|tmpfs|cdrom' | grep $(readlink -f /dev/sdf) | awk '{ print $5 " " $1 }' | while read output;
                do
                  echo $output
                  usep=$(echo $output | awk '{ print $1}' | cut -d'%' -f1  )
                  partition=$(echo $output | awk '{ print $2 }' )
                  if [ $usep -ge 90 ]; then
                    if [ "${AggregatedNotifications}" != "" ]
                    then
                      aws sns publish --topic-arn=${AggregatedNotifications} --subject="${AWS::StackName} - Upsizing Snapshot Size" --message="The snapshot for ${AWS::StackName} has reached $usep%. Resizing to $NEW_DISK_SIZE GB."
                      if [ "${AlarmSNSTopic}" != "" ]
                      then
                        aws sns publish --topic-arn=${AlarmSNSTopic} --subject="${AWS::StackName} - Upsizing Snapshot Size" --message="The snapshot for ${AWS::StackName} has reached $usep%. Resizing to $NEW_DISK_SIZE GB."
                      fi
                      PARAMETERS_RESIZE=$(echo "$PARAMETERS" | jq 'map(if .ParameterKey == "DiskSize" then .ParameterValue="'$NEW_DISK_SIZE'" else . end)' -c)
                      aws cloudformation update-stack --stack-name ${AWS::StackName} --use-previous-template --capabilities CAPABILITY_IAM --parameters="$PARAMETERS_RESIZE"
                    fi
                    else
                      aws cloudformation update-stack --stack-name ${AWS::StackName} --use-previous-template --capabilities CAPABILITY_IAM --parameters="$PARAMETERS"
                  fi
                done


                # IF Day of week is Sunday, compact DB and re-update everything, after taking a new snapshot.
                if [ $(date +%u) = 7 ]; then
                  geth compactdb --datadir=/var/lib/ethereum ${FreezerFlags}
                  NEW_SNAPSHOT_ID=`aws ec2 create-snapshot --volume-id $VOLUME_ID --tag-specification="ResourceType=snapshot,Tags=[{Key=cluster,Value=${KafkaTopic}},{Key=Name,Value=${KafkaTopic}-chaindata-$(date -Isecond -u)}]" | jq '.SnapshotId' -cr`
                  echo "Waiting for snapshot to complete"
                  while [ `aws ec2 describe-snapshots --filters=Name=snapshot-id,Values=$NEW_SNAPSHOT_ID | jq '.Snapshots[0].State' -cr` != "completed" ];
                  do
                      sleep 10
                  done

                  # CFN will set any parameters we don't provide back to their default values,
                  # so get all of the parameters, update SnapshotID, and update the stack with
                  # the new parameters.
                  PARAMETERS=$(aws cloudformation describe-stacks --stack-name ${AWS::StackName} | jq '.Stacks[0].Parameters | map(if .ParameterKey == "SnapshotId" then .ParameterValue="'$NEW_SNAPSHOT_ID'" else . end) | map(if .ParameterKey == "SnapshotTimestamp" then .ParameterValue="'$(date +%s)'" else . end)' -c)
                  aws cloudformation update-stack --stack-name ${AWS::StackName} --use-previous-template --capabilities CAPABILITY_IAM --parameters="$PARAMETERS"

                  #DELETE OLD SNAPSHOT ONCE EVERYTHING UPDATED
                  aws ec2 delete-snapshot --snapshot-id $SNAPSHOT_ID
                fi
                  if [ "${S3BackupLogsBucket}" != "" ]
                  then
                    cat /var/log/cloud-init-output.log | aws s3 cp - s3://${S3BackupLogsBucket}/ethercattle/${NetworkId}/success-$(date '+%Y%m%d-%H%M%S').log || true
                  fi
                poweroff
              - SnapshotterNodeRole: !Ref SnapshotterNodeRole
                ClusterId:
                  "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]
                KafkaTopicString: !If [ HasSnapshotOffset, !Sub "${KafkaTopic}:${SnapshotOffset}", !Ref KafkaTopic ]

                # Goes Above at Placeholder
                # # Check and see if all blocks that were pulled down are actually there and not returning null.
                # sudo -Eu geth /usr/bin/geth ${MasterExtraFlags} --rpc.allow-unprotected-txs --snapshot=false --syncmode=fast --txlookuplimit=0 --cache=$allocatesafe --light.maxpeers 0 --maxpeers 0 --http --http.addr 0.0.0.0 --http.port 8545 --datadir=/var/lib/ethereum ${FreezerFlags} &
                # pid=$!
                # test_pass=0
                # while [ $test_pass -le 1 ]
                # do
                #   if curl -X POST --insecure -H "Content-Type: application/json" --data '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":64}' localhost:8545
                #   then
                #     test_pass=$(( $test_pass + 1 ))
                #   else
                #     test_pass=0
                #   fi
                #   sleep 30
                # done
                # END_BLOCK=$(curl -X POST --insecure -H "Content-Type: application/json" --data '{"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":64}' localhost:8545 | jq -r '.result')
                # for (( block=START_BLOCK; block<=END_BLOCK; block++)); do
                #   res=$(curl -s -X POST --compressed --insecure --compressed -H "Content-Type: application/json" --data '{"jsonrpc":"2.0","method":"eth_getBlockByNumber","params":["'$(printf "%#x" $block)'", false],"id":1}' localhost:8545)
                #   if [ "$(echo $res | jq '.result')" = "null" ]
                #   then
                #     if [ "${AggregatedNotifications}" != "" ]
                #     then
                #       aws sns publish --topic-arn=${AggregatedNotifications} --subject="${AWS::StackName} - Snapshotting Failed" --message="The snapshotting process for ${KafkaTopic} could not validate all blocks from $START_BLOCK to $END_BLOCK, and failed on $block with $res"
                #     fi
                #     if [ "${AlarmSNSTopic}" != "" ]
                #     then
                #       aws sns publish --topic-arn=${AlarmSNSTopic} --subject="${AWS::StackName} - Snapshotting Failed" --message="The snapshotting process for ${KafkaTopic} could not validate all blocks from $START_BLOCK to $END_BLOCK, and failed on $block with $res"
                #     fi
                #     if [ "${S3BackupLogsBucket}" != "" ]
                #     then
                #       cat /var/log/cloud-init-output.log | aws s3 cp - s3://${S3BackupLogsBucket}/ethercattle/${NetworkId}/failure-$(date '+%Y%m%d-%H%M%S').log || true
                #     fi
                #     poweroff
                #   fi
                # done
                #
                # sudo pkill -f geth
                # wait $pid

  PrunerLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateData:
        ImageId: !If [HasReplicaImageAMI, !Ref ReplicaImageAMI, !FindInMap [RegionMap, !Ref "AWS::Region", AL2AMI]]
        InstanceType: m5a.large
        TagSpecifications:
          - ResourceType: instance
            Tags:
              - Key: Name
                Value: !Sub "${KafkaTopic}-Pruner"
          - ResourceType: volume
            Tags:
              - Key: Name
                Value: !Sub "${KafkaTopic}-Pruner"
        SecurityGroupIds:
          - !Sub ${ReplicaNodeSecurityGroup.GroupId}
        IamInstanceProfile:
          Name: !Ref SnapshotterNodeInstanceProfile
        KeyName: !If [HasKeyName, !Ref KeyName, !Ref 'AWS::NoValue']
        InstanceInitiatedShutdownBehavior: terminate
        BlockDeviceMappings:
        - DeviceName: "/dev/xvda"
          Ebs:
            VolumeSize: 8
            VolumeType: gp2
        - DeviceName: "/dev/sds"
          Ebs:
            VolumeSize: 8
            VolumeType: gp3
        - DeviceName: "/dev/sdg"
          Ebs:
            VolumeSize: !Ref PrunedDiskSize
            VolumeType: gp3
        UserData:
          "Fn::Base64":
            "Fn::Sub":
              - |
                #!/bin/bash -xe
                if [ "$(arch)" == "x86_64" ]
                then
                  ARCH="amd64"
                elif [ "$(arch)" == "aarch64" ]
                then
                  ARCH="arm64"
                fi

                totalm=$(free -m | awk '/^Mem:/{print $2}') ; echo $totalm
                export allocatesafe=$((totalm * 75 / 100))

                sysctl -p || true
                GETH_BIN="geth-linux-$ARCH"
                aws s3 cp s3://${S3GethBucketName}/${ECGethVersion}/$GETH_BIN /usr/bin/geth
                chmod +x /usr/bin/geth

                ## STEP 1: Mount both volumes

                export AWS_DEFAULT_REGION=${AWS::Region}
                export AWS_REGION=${AWS::Region}

                AVAILABILITY_ZONE=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)
                INSTANCE_ID=$(curl -s http://169.254.169.254/2012-01-12/meta-data/instance-id)

                SOURCE_VOLUME_ID=$(aws ec2 describe-volumes --filters Name=tag:SnapshotVolume,Values=${AWS::StackName}-${AWS::Region}-${PrunerGeneration} Name=status,Values=available Name=availability-zone,Values=$AVAILABILITY_ZONE | jq -r .Volumes[0].VolumeId)

                if [ "$SOURCE_VOLUME_ID" == "null"  ]
                then
                  if [ "${AggregatedNotifications}" != "" ]
                  then
                    aws sns publish --topic-arn=${AggregatedNotifications} --subject="${AWS::StackName} - Pruning Failed" --message="The pruning for ${AWS::StackName} failed due to no available source volumes."
                    if [ "${AlarmSNSTopic}" != "" ]
                    then
                      aws sns publish --topic-arn=${AlarmSNSTopic} --subject="${AWS::StackName} - Pruning Failed" --message="The pruning for ${AWS::StackName} failed due to no available source volumes."
                    fi
                  fi
                  poweroff
                fi
                DEST_VOLUME_ID=$(aws ec2 describe-volumes --filters Name=attachment.instance-id,Values="$INSTANCE_ID" | jq '.Volumes[] | select(. | .Attachments[0].Device == "/dev/sdg") | .VolumeId' -cr)
                aws ec2 attach-volume --device=/dev/sdf --instance-id=$INSTANCE_ID --volume-id=$SOURCE_VOLUME_ID

                while [ "$(aws ec2 describe-volumes --volume-id $SOURCE_VOLUME_ID | jq -r .Volumes[0].State)" != "in-use" ]
                do
                  sleep 3
                done

                mkfs.ext4 /dev/sdg

                mkdir -p /var/lib/ethereum
                mkdir -p /var/lib/ethereum-pruned

                ignore="$(readlink -f /dev/sd*) $(readlink -f /dev/xvd*)"
                cutignore="$(for x in $ignore ; do echo $x | cut -c -12; done | uniq)"
                devices="$(ls /dev/nvme* | grep -E 'n1$')" || devices=""
                cutdevices="$(for x in $devices ; do echo $x | cut -c -12; done | uniq)"
                localnvme=$(for d in $cutdevices; do if ! $(echo "$cutignore"| grep -q $d) ; then echo $d; fi ; done)
                if [ ! -z "$localnvme" ]
                then
                  # If there is a localnvme volume, copy everything from the old snapshot volume to the localnvme volume.
                  # Pruning goes much faster from NVME volumes due to low latency.
                  mkfs.ext4 $localnvme
                  mount -o barrier=0,data=writeback $localnvme /var/lib/ethereum
                  mkdir -p /var/lib/ethereum-slow
                  mount -o barrier=0,data=writeback /dev/sdf /var/lib/ethereum-slow
                  cp -r /var/lib/ethereum-slow/* /var/lib/ethereum
                else
                  mount -o barrier=0,data=writeback /dev/sdf /var/lib/ethereum
                fi

                mount -o barrier=0,data=writeback /dev/sdg /var/lib/ethereum-pruned

                resize2fs /dev/sdf
                useradd -r geth
                chown -R geth /var/lib/ethereum

                cp -a /var/lib/ethereum/keystore /var/lib/ethereum-pruned/keystore || true
                cp -a /var/lib/ethereum/geth/keystore /var/lib/ethereum-pruned/geth/keystore || true
                cp -a /var/lib/ethereum/geth/ethash /var/lib/ethereum-pruned/geth/ethash || true
                cp -a /var/lib/ethereum/geth/nodes /var/lib/ethereum-pruned/geth/nodes || true

                yum install -y jq

                printf "ReplicaClusterVersion=${ReplicaClusterVersion}\nKafkaHostname=${KafkaBrokerURL}\nKafkaTopic=${KafkaTopic}\nNetworkId=${NetworkId}\ninfraName=${InfrastructureStack}\nAWS_REGION=${AWS::Region}\n" > /etc/systemd/system/ethcattle-vars

                if [ -f /usr/bin/replica-snapshot-hook ]
                then
                  /usr/bin/replica-snapshot-hook
                fi

                sysctl -w fs.file-max=12000500
                sysctl -w fs.nr_open=20000500
                ulimit -n 20000000

                SEP=$(echo "${KafkaBrokerURL}" | grep -q "?" && echo "&" || echo "?")
                SEP_ESCAPED=$(echo "${KafkaBrokerURL}" | grep -q "?" && echo "\\&" || echo "?")
                KAFKA_ESCAPED_URL="$(printf "${KafkaBrokerURL}"| sed 's^&^\\&^g')"
                /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource SnapshotterLaunchTemplate --configsets s_scripting --region ${AWS::Region} --role ${SnapshotterNodeRole}


                ## STEP 2: Sync up to date so we're starting with the latest available data

                sudo -Eu geth /usr/bin/geth replica --snapshot=false --cache=$allocatesafe ${ReplicaExtraFlags} ${FreezerFlags} --kafka.broker=$KAFKA_ESCAPED_URL""$SEP""avoid_leader=1 --datadir=/var/lib/ethereum --kafka.topic=${KafkaTopic} --replica.syncshutdown
                if ! sudo -Eu geth /usr/bin/geth verifystatetrie ${FreezerFlags} --datadir=/var/lib/ethereum ${SnapshotValidationThreshold}
                then
                  if [ "${AggregatedNotifications}" != "" ]
                  then
                    aws sns publish --topic-arn=${AggregatedNotifications} --subject="${AWS::StackName} - Bad State Trie" --message="State trie verification failed while taking snapshot for cluster '${KafkaTopic}'. No snapshot will be taken. This is probably unrecoverable, and you will need to deploy a new cluster from your last good snapshot (probably '${SnapshotId}')"
                  fi
                  if [ "${AlarmSNSTopic}" != "" ]
                  then
                    aws sns publish --topic-arn=${AlarmSNSTopic} --subject="${AWS::StackName} - Bad State Trie" --message="State trie verification failed while taking snapshot for cluster '${KafkaTopic}'. No snapshot will be taken. This is probably unrecoverable, and you will need to deploy a new cluster from your last good snapshot (probably '${SnapshotId}')"
                  fi
                  if [ "${S3BackupLogsBucket}" != "" ]
                  then
                    cat /var/log/cloud-init-output.log | aws s3 cp - s3://${S3BackupLogsBucket}/ethercattle/${NetworkId}/pruner-failure-$(date '+%Y%m%d-%H%M%S').log || true
                  fi
                  poweroff
                fi

                ## STEP 3: Migrate data to the new volume. This will take a loooooooooooonnnnnng time (days).

                mkdir -p /var/lib/ethereum-pruned/geth/chaindata
                chown -R geth /var/lib/ethereum-pruned
                chown -R geth /var/lib/ethereum

                fio --filename=/dev/sdf --rw=read --bs=128k --iodepth=32 --ioengine=libaio --prio=7 --prioclass=3 --thinktime=2 --rate_iops=4000 --direct=1 --name=volume-initialize &

                if ! sudo -Eu geth /usr/bin/geth migratestate ${FreezerLocation} /var/lib/ethereum/geth/chaindata /var/lib/ethereum-pruned/geth/chaindata ${KafkaTopic}
                then
                  if [ "${AggregatedNotifications}" != "" ]
                  then
                    aws sns publish --topic-arn=${AggregatedNotifications} --subject="${AWS::StackName} - State Migration Failed" --message="Migrating the state to a new volume failed.')"
                  fi
                  if [ "${AlarmSNSTopic}" != "" ]
                  then
                    aws sns publish --topic-arn=${AlarmSNSTopic} --subject="${AWS::StackName} - Bad State Trie" --message="Migrating the state to a new volume failed.')"
                  fi
                  if [ "${S3BackupLogsBucket}" != "" ]
                  then
                    cat /var/log/cloud-init-output.log | aws s3 cp - s3://${S3BackupLogsBucket}/ethercattle/${NetworkId}/pruner-failure-$(date '+%Y%m%d-%H%M%S').log || true
                  fi
                  poweroff
                fi
                cp -r ${FreezerLocation} ${PrunedFreezerLocation} || true
                chown -R geth /var/lib/ethereum-pruned/


                ## STEP 4: Sync the new volume up to date so we're snapshotting the latest available data

                sudo -Eu geth /usr/bin/geth replica --snapshot=false --cache=$allocatesafe ${ReplicaExtraFlags} --datadir.ancient=${PrunedFreezerLocation} --kafka.broker=$KAFKA_ESCAPED_URL""$SEP""avoid_leader=1 --datadir=/var/lib/ethereum-pruned --kafka.topic=${KafkaTopic} --replica.syncshutdown
                if ! sudo -Eu geth /usr/bin/geth verifystatetrie ${FreezerFlags} --datadir=/var/lib/ethereum-pruned ${SnapshotValidationThreshold}
                then
                  if [ "${AggregatedNotifications}" != "" ]
                  then
                    aws sns publish --topic-arn=${AggregatedNotifications} --subject="${AWS::StackName} - Bad State Trie" --message="State trie verification failed while taking snapshot for cluster '${KafkaTopic}'. No snapshot will be taken. This is probably unrecoverable, and you will need to deploy a new cluster from your last good snapshot (probably '${SnapshotId}')"
                  fi
                  if [ "${AlarmSNSTopic}" != "" ]
                  then
                    aws sns publish --topic-arn=${AlarmSNSTopic} --subject="${AWS::StackName} - Bad State Trie" --message="State trie verification failed while taking snapshot for cluster '${KafkaTopic}'. No snapshot will be taken. This is probably unrecoverable, and you will need to deploy a new cluster from your last good snapshot (probably '${SnapshotId}')"
                  fi
                  if [ "${S3BackupLogsBucket}" != "" ]
                  then
                    cat /var/log/cloud-init-output.log | aws s3 cp - s3://${S3BackupLogsBucket}/ethercattle/${NetworkId}/pruner-failure-$(date '+%Y%m%d-%H%M%S').log || true
                  fi
                  poweroff
                fi


                ## STEP 5: Take the snapshot of the pruned volume

                SNAPSHOT_ID=`aws ec2 create-snapshot --volume-id $DEST_VOLUME_ID --tag-specification="ResourceType=snapshot,Tags=[{Key=cluster,Value=${KafkaTopic}},{Key=Name,Value=${KafkaTopic}-chaindata-$(date -Isecond -u)}]" | jq '.SnapshotId' -cr`

                if [ "$SNAPSHOT_ID" == "null" ]; then
                  if [ "${AggregatedNotifications}" != "" ]
                  then
                    aws sns publish --topic-arn=${AggregatedNotifications} --subject="${AWS::StackName} - Pruning Volume Failed" --message="The snapshotting process for ${KafkaTopic} failed to take a snapshot."
                  fi
                  if [ "${AlarmSNSTopic}" != "" ]
                  then
                    aws sns publish --topic-arn=${AlarmSNSTopic} --subject="${AWS::StackName} - Pruning Volume Failed" --message="The snapshotting process for ${KafkaTopic} failed to take a snapshot."
                  fi
                  if [ "${S3BackupLogsBucket}" != "" ]
                  then
                    cat /var/log/cloud-init-output.log | aws s3 cp - s3://${S3BackupLogsBucket}/ethercattle/${NetworkId}/failure-$(date '+%Y%m%d-%H%M%S').log || true
                  fi
                  poweroff
                fi
                echo "Waiting for snapshot to complete"
                while [ `aws ec2 describe-snapshots --filters=Name=snapshot-id,Values=$SNAPSHOT_ID | jq '.Snapshots[0].State' -cr` != "completed" ];
                do
                    sleep 10
                done

                ## STEP 6: Make sure this is the volume that will be used for the next snapshot

                umount /var/lib/ethereum/
                umount /var/lib/ethereum-pruned
                mount /dev/sdg /var/lib/ethereum

                ## STEP 7: Before we update cloudformation, ensure this snapshot is valid by running it for a few blocks

                # After the snapshot completes, sync from the network. This is an extra integrity
                # check on the snapshot, as it requires it to be able to validate blocks.
                # We don't want this to end up in our final snapshot, but if we can't sync from
                # the network we don't want to present this snapshot to the cluster.
                if ! /tmp/validate_block_catchup.sh
                then
                  if [ "${AggregatedNotifications}" != "" ]
                  then
                    aws sns publish --topic-arn=${AggregatedNotifications} --subject="${AWS::StackName} - Snapshotting Volume Failed" --message="The snapshotting process for ${KafkaTopic} took a snapshot, but Geth could not sync. Not updating CloudFormation"
                  fi
                  if [ "${AlarmSNSTopic}" != "" ]
                  then
                    aws sns publish --topic-arn=${AlarmSNSTopic} --subject="${AWS::StackName} - Snapshotting Volume Failed" --message="The snapshotting process for ${KafkaTopic} took a snapshot, but Geth could not sync. Not updating CloudFormation"
                  fi
                  if [ "${S3BackupLogsBucket}" != "" ]
                  then
                    cat /var/log/cloud-init-output.log | aws s3 cp - s3://${S3BackupLogsBucket}/ethercattle/${NetworkId}/failure-$(date '+%Y%m%d-%H%M%S').log || true
                  fi
                  poweroff
                fi

                # CFN will set any parameters we don't provide back to their default values,
                # so get all of the parameters, update SnapshotID, and update the stack with
                # the new parameters.
                PARAMETERS=$(aws cloudformation describe-stacks --stack-name ${AWS::StackName} | jq '.Stacks[0].Parameters | map(if .ParameterKey == "SnapshotId" then .ParameterValue="'$SNAPSHOT_ID'" else . end) | map(if .ParameterKey == "SnapshotTimestamp" then .ParameterValue="'$(date +%s)'" else . end)' -c)

                # Resize Disks
                ORIGINAL_DISK_SIZE=`aws ec2 describe-volumes --volume-id "$DEST_VOLUME_ID" | jq '.Volumes[] | .Size' -cr`
                NEW_DISK_SIZE=$((ORIGINAL_DISK_SIZE*115/100))

                ## manually grepping for sdf (mount directory above) because we don't really need to worry about the others.
                df -H | grep -vE '^Filesystem|tmpfs|cdrom' | grep $(readlink -f /dev/sdg) | awk '{ print $5 " " $1 }' | while read output;
                do
                  echo $output
                  usep=$(echo $output | awk '{ print $1}' | cut -d'%' -f1  )
                  partition=$(echo $output | awk '{ print $2 }' )
                  if [ $usep -ge 90 ]; then
                    PARAMETERS_RESIZE=$(echo "$PARAMETERS" | jq 'map(if .ParameterKey == "DiskSize" then .ParameterValue="'$NEW_DISK_SIZE'" else . end)' -c)
                  else
                    PARAMETERS_RESIZE=$(echo "$PARAMETERS" | jq 'map(if .ParameterKey == "DiskSize" then .ParameterValue="'$ORIGINAL_DISK_SIZE'" else . end)' -c)
                  fi
                  PARAMETERS_FINAL=$(echo "$PARAMETERS_RESIZE" | jq 'map(if .ParameterKey == "PrunerGeneration" then .ParameterValue="'${NextPrunerGeneration.Sum}'" else . end)' -c)
                  aws cloudformation update-stack --stack-name ${AWS::StackName} --use-previous-template --capabilities CAPABILITY_IAM --parameters="$PARAMETERS_FINAL"
                done

                if [ "${AggregatedNotifications}" != "" ]
                then
                  aws sns publish --topic-arn=${AggregatedNotifications} --subject="${AWS::StackName} - Pruning Complete" --message="The pruning process for ${KafkaTopic} has completed. It is important that at least one master for this stack be restarted soon, but that requires manual intervention."
                fi
                if [ "${AlarmSNSTopic}" != "" ]
                then
                  aws sns publish --topic-arn=${AlarmSNSTopic} --subject="${AWS::StackName} - Pruning Complete" --message="The pruning process for ${KafkaTopic} has completed. It is important that at least one master for this stack be restarted soon, but that requires manual intervention."
                fi


                if [ "${S3BackupLogsBucket}" != "" ]
                then
                  aws s3 cp /var/log/cloud-init-output.log s3://${S3BackupLogsBucket}/ethercattle/${NetworkId}/pruner-success-$(date '+%Y%m%d-%H%M%S').log || true
                fi
                poweroff
              - SnapshotterNodeRole: !Ref SnapshotterNodeRole
                ClusterId:
                  "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]
                FreezerLocation: !If [ EnabledFreezerBucket, {"Fn::Sub": ["s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, "/var/lib/ethereum/geth/chaindata/ancient"]
                PrunedFreezerLocation: !If [ EnabledFreezerBucket, {"Fn::Sub": ["s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, "/var/lib/ethereum-pruned/geth/chaindata/ancient"]

  FallBackLG:
    Type: AWS::Logs::LogGroup
    Properties:
      RetentionInDays: 7
      LogGroupName:
        "Fn::Sub":
          - "/${ClusterId}/${AWS::StackName}/fallback"
          - ClusterId:
              "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"

  FallbackLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateData:
        ImageId: !If [HasReplicaImageAMI, !Ref ReplicaImageAMI, !FindInMap [RegionMap, !Ref "AWS::Region", AL2AMI]]
        InstanceType: m5ad.large
        TagSpecifications:
          - ResourceType: instance
            Tags:
              - Key: Name
                Value: !Sub "${KafkaTopic}-Fallback"
          - ResourceType: volume
            Tags:
              - Key: Name
                Value: !Sub "${KafkaTopic}-Fallback"
        SecurityGroupIds:
          - !Sub ${ReplicaNodeSecurityGroup.GroupId}
          - !Sub ${MasterNodeSecurityGroup.GroupId}
          - !If [HasExtraSecurityGroup, !Ref ReplicaExtraSecurityGroup, !Ref 'AWS::NoValue']
        IamInstanceProfile:
          Name: !Ref ReplicaNodeInstanceProfile
        KeyName: !If [HasKeyName, !Ref KeyName, !Ref 'AWS::NoValue']
        # CreditSpecification: !If [SmallReplica, {CpuCredits: standard}, !Ref 'AWS::NoValue']
        BlockDeviceMappings:
        - DeviceName: "/dev/xvda"
          Ebs:
            VolumeSize: 8
            VolumeType: gp2
        - DeviceName: "/dev/sds"
          Ebs:
            VolumeSize: 8
            VolumeType: gp3
        - DeviceName: "/dev/sdf"
          Ebs:
            VolumeSize: !If [ReplicaHDD, !GetAtt HDDSize.Value, !Ref DiskSize]
            VolumeType: !If [ReplicaHDD, !Ref ReplicaDiskType, "io1"]
            # Iops: !If [ReplicaHDD, !Ref 'AWS::NoValue', 3000 ]
            # Throughput: !If [ReplicaHDD, !Ref 'AWS::NoValue', 125 ]
            # VolumeType: !If [ReplicaHDD, !Ref ReplicaDiskType, "gp3"]
            Iops: !If [ReplicaHDD, !Ref 'AWS::NoValue', !GetAtt VolumeIOPS.Value ]
            # Throughput: !If [ReplicaHDD, !Ref 'AWS::NoValue', 1000 ]
            SnapshotId: !Ref SnapshotId
        - !If [ SmallReplica, {DeviceName: "/dev/sdg", Ebs: {VolumeSize: 25, VolumeType: gp3, Iops: 3000, Throughput: 125}}, {DeviceName: "/dev/sdg", Ebs: {VolumeSize: 150, VolumeType: gp3, Iops: 3000, Throughput: 125}}]
        UserData:
          "Fn::Base64":
            "Fn::Sub":
              - |
                #!/bin/bash -xe
                /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource MasterLaunchTemplate --configsets setup --region ${AWS::Region} || (sleep 30; /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource MasterLaunchTemplate --configsets setup --region ${AWS::Region} || poweroff)
                /root/service-configs.sh || (sleep 30; /root/service-configs.sh || poweroff)
                /root/configure-server.sh || (sleep 30; /root/configure-server.sh || poweroff)
                /root/start-fallback.sh || (sleep 30; /root/start-fallback.sh || poweroff)
                /root/loadtest-disk.sh || poweroff
                export AWS_DEFAULT_REGION=${AWS::Region}
                VOLUME_ID=$(aws ec2 describe-volumes --filters Name=attachment.instance-id,Values="$(curl http://169.254.169.254/latest/meta-data/instance-id)" | jq '.Volumes[] | select(. | .Attachments[0].Device == "/dev/sdf") | .VolumeId' -cr)
                /usr/local/bin/aws ec2 modify-volume --volume-id $VOLUME_ID --volume-type gp3 --iops 3000 --throughput 125 &
              - ClusterId:
                  "Fn::ImportValue": !Sub "${InfrastructureStack}-ClusterId"
                ReplicaHTTPFlag:
                    !If [HasReplicaHTTP, "--http --http.addr 0.0.0.0 --http.port 8545", ""]
                ReplicaGraphQLFlag:
                    # Wow that's a lot of escaping!
                    !If [HasReplicaGraphQL, "--graphql --graphql.addr 0.0.0.0 --graphql.port 8547 --graphql.vhosts \\\\\\'*\\\\\\'", ""]
                ReplicaWebsocketsFlag:
                    !If [HasReplicaWebsockets, "--ws --ws.addr 0.0.0.0 --ws.port 8546", ""]
                BaseInfrastructure:
                  "Fn::ImportValue": !Sub "${InfrastructureStack}-BaseInfrastructure"
                FreezerFlags: !If [ EnabledFreezerBucket, {"Fn::Sub": ["--datadir.ancient=s3://${Bucket}/${NetworkId}", {Bucket: !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]} ]}, ""]

  FallbackAutoScalingPolicy:
    Type: AWS::AutoScaling::ScalingPolicy
    Properties:
      # AdjustmentType: String
      AutoScalingGroupName: !Ref FallbackAutoScalingGroup
      # Cooldown: 900
      EstimatedInstanceWarmup: 600
      # MetricAggregationType: String
      # MinAdjustmentMagnitude: Integer
      PolicyType: TargetTrackingScaling
      # ScalingAdjustment: Integer
      # StepAdjustments:
      #   - StepAdjustment
      TargetTrackingConfiguration:
        PredefinedMetricSpecification:
          PredefinedMetricType: ASGAverageCPUUtilization
        TargetValue: !Ref ReplicaCPUScalingTargetValue

  FallbackAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      VPCZoneIdentifier:
        - "Fn::ImportValue":
            !Sub "${InfrastructureStack}-PublicA"
        - "Fn::ImportValue":
            !Sub "${InfrastructureStack}-PublicB"
        - "Fn::ImportValue":
            !Sub "${InfrastructureStack}-PublicC"
      MixedInstancesPolicy:
        InstancesDistribution:
          OnDemandPercentageAboveBaseCapacity: !Ref FallbackOnDemandPercentage
          SpotAllocationStrategy: !Ref ReplicaSpotAllocationStrategy
          SpotInstancePools: !If [ ReplicaSpotLowestPrice, !FindInMap [PoolSize, Size, !Ref ReplicaSize], !Ref "AWS::NoValue"]
        LaunchTemplate:
          LaunchTemplateSpecification:
            LaunchTemplateId: !Ref FallbackLaunchTemplate
            Version: !Sub ${FallbackLaunchTemplate.LatestVersionNumber}
          Overrides: !FindInMap [InstanceSizes, Fallback, !Ref ReplicaSize]
      MinSize: !Ref FallbackTargetCapacity
      MaxSize: !Ref FallbackMaxCapacity
      HealthCheckType: EC2
      CapacityRebalance: false
      TargetGroupARNs: !If [NoTG, !Ref "AWS::NoValue", !Split [ ",", !If [HasATG, !Ref AlternateTargetGroup, {"Fn::ImportValue": !Sub "${InfrastructureStack}-ALBGroupList"}]]]
      MetricsCollection:
      - Granularity: 1Minute
        Metrics:
        - GroupInServiceInstances
      Tags:
      - Key: Name
        Value: !Sub ${AWS::StackName}-Fallback
        PropagateAtLaunch: 'true'

  SnapshotterNodeRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - ec2.amazonaws.com
            - autoscaling.amazonaws.com
        Version: '2012-10-17'
  SnapshotterNodePolicy:
    Type: "AWS::IAM::Policy"
    Properties:
      Roles:
        - !Ref SnapshotterNodeRole
      PolicyName: !Sub "SnapshotterNode${AWS::StackName}"
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Action:
              - s3:GetObject
            Resource: !Sub arn:aws:s3:::${S3GethBucketName}/*
            Effect: Allow
          - Action:
              - s3:GetObject
              - s3:ListBucket
              - s3:GetBucketPolicy
              - s3:GetObjectTagging
              - s3:GetBucketLocation
              - s3:PutObject
            Resource: !Sub arn:aws:s3:::${S3BackupLogsBucket}/*
            Effect: Allow
          - Action:
              - lambda:GetFunctionConfiguration
              - lambda:InvokeAsync
              - lambda:InvokeFunction
              - lambda:GetFunction
              - iam:ListRoles
              - iam:ListRolePolicies
              - iam:GetRole
              - iam:PassRole
              - iam:GetRolePolicy
              - iam:ListAttachedRolePolicies
              - cloudwatch:PutMetricAlarm
            Resource:
              - "*"
            Effect: Allow
          - Action:
              - s3:GetObject
              - s3:PutObject
              - s3:ListBucket
              - s3:GetBucketPolicy
              - s3:GetObjectTagging
              - s3:GetBucketLocation
            Resource:
              - {"Fn::Sub": ["arn:aws:s3:::${Bucket}/*", {"Bucket": !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]}]}
              - {"Fn::Sub": ["arn:aws:s3:::${Bucket}", {"Bucket": !If [ HasFreezerBucket, !Ref FreezerBucket, {"Fn::ImportValue": !Sub "${InfrastructureStack}-FreezerBucket"} ]}]}
            Effect: Allow
          - Action:
              - cloudformation:UpdateStack
            Resource: !Sub "arn:aws:cloudformation:${AWS::Region}:${AWS::AccountId}:stack/${AWS::StackName}/*"
            Effect: Allow
          - Action:
              - iam:GetInstanceProfile
            Resource:
              - !Sub ${MasterNodeInstanceProfile.Arn}
              - !Sub ${ReplicaNodeInstanceProfile.Arn}
              - !Sub ${SnapshotterNodeInstanceProfile.Arn}
            Effect: Allow
          - Action:
              - lambda:UpdateFunctionConfiguration
              - lambda:GetFunctionConfiguration
            Resource:
              - !Sub ${SnapshotterLambdaFunction.Arn}
              - !Sub ${PrunerLambdaFunction.Arn}
              - !Sub ${SnapshotGCLambdaFunction.Arn}
              - !Sub ${DiskSizeLambdaFunction.Arn}
              - !Sub ${SnapDiskSizeLambdaFunction.Arn}
            Effect: Allow
          - Action:
              - iam:PassRole
              - iam:GetRole
              - iam:PutRolePolicy
            Resource:
              - !Sub "${ReplicaNodeRole.Arn}"
              - !Sub "${MasterNodeRole.Arn}"
              - !Sub "${SnapshotterLambdaRole.Arn}"
              - !Sub "${SnapshotterNodeRole.Arn}"
              - !Sub "${SnapshotGCLambdaRole.Arn}"
            Effect: Allow
          - Action:
              - sns:Publish
            Resource:
              - !Ref AggregatedNotifications
              - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
            Effect: Allow
          - Action:
              - autoscaling:EnableMetricsCollection
              - autoscaling:DisableMetricsCollection
              - autoscaling:UpdateAutoScalingGroup
            Resource:
              - "*"
            Condition:
              StringEquals:
                "autoscaling:ResourceTag/aws:cloudformation:stack-id": !Sub "${AWS::StackId}"
            Effect: Allow
          - Action:
              - cloudformation:DescribeStacks
              - cloudformation:DescribeStackResource
              - ec2:DescribeLaunchTemplates
              - ec2:DescribeSnapshotAttribute
              - ec2:CreateTags
              - ec2:DeleteTags
              - ec2:DescribeLaunchTemplateVersions
              - ec2:RunInstances
              - ec2:DescribeSnapshots
              - ec2:CreateLaunchTemplateVersion
              - ec2:DescribeVolumeStatus
              - autoscaling:DescribeAutoScalingGroups
              - autoscaling:DescribeScalingActivities
              - ec2:DescribeVolumes
              - ec2:CreateVolume
              - ec2:AttachVolume
              - ec2:DetachVolume
              - ec2:CreateSnapshot
              - ec2:DeleteSnapshot
              - events:DescribeRule
              - ec2:DescribeKeyPairs
            Resource: "*"
            Effect: Allow
  SnapshotterNodeInstanceProfile:
    Type: "AWS::IAM::InstanceProfile"
    Properties:
      Path: /
      Roles:
      - !Ref SnapshotterNodeRole
    DependsOn: SnapshotterNodeRole
  SnapshotterLambdaRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
        Version: '2012-10-17'
  SnapshotterLambdaPolicy:
    Type: "AWS::IAM::Policy"
    Properties:
      Roles:
        - !Ref SnapshotterLambdaRole
      PolicyName: !Sub "SnapshotterLambdaPolicy${AWS::StackName}"
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Action:
              - logs:CreateLogStream
              - logs:CreateLogGroup
              - logs:PutLogEvents
              - sns:Publish
            Effect: Allow
            Resource: "*"
          - Effect: Allow
            Action:
              - iam:PassRole
            Resource: !Sub "${SnapshotterNodeRole.Arn}"
          - Effect: Allow
            Action:
              - ec2:CreateTags
              - ec2:RunInstances
            Resource:
              - Fn::Sub:
                - "arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:subnet/${PublicA}"
                - PublicA:
                    "Fn::ImportValue":
                      !Sub "${InfrastructureStack}-PublicA"
              - Fn::Sub:
                - "arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:subnet/${PublicB}"
                - PublicB:
                    "Fn::ImportValue":
                      !Sub "${InfrastructureStack}-PublicB"
              - Fn::Sub:
                - "arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:subnet/${PublicC}"
                - PublicC:
                    "Fn::ImportValue":
                      !Sub "${InfrastructureStack}-PublicC"
              - !Sub "arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:key-pair/${KeyName}"
              - !Sub "arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:instance/*"
              - !Sub "arn:aws:ec2:*::snapshot/${SnapshotId}"
              - !Sub "arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:volume/*"
              - !Sub "arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:security-group/${ReplicaNodeSecurityGroup.GroupId}"
              - !Sub "arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:network-interface/*"
              - !Sub "arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:launch-template/${SnapshotterLaunchTemplate}"
              - !Sub "arn:aws:ec2:${AWS::Region}:${AWS::AccountId}:launch-template/${PrunerLaunchTemplate}"
              - "arn:aws:ec2:*::image/*"

  SnapshotterLambdaFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Code:
        S3Bucket: !Ref S3GethBucketName
        S3Key: lambdaPackage-31.zip
      Description: "Launch instances to snapshot chaindata"
      Environment:
        Variables:
          LAUNCH_TEMPLATE_ID: !Ref SnapshotterLaunchTemplate
          LAUNCH_TEMPLATE_VERSION: !Sub "${SnapshotterLaunchTemplate.LatestVersionNumber}"
          SUBNET_ID:
            "Fn::ImportValue":
                !Sub "${InfrastructureStack}-PublicA"
          VOLUME_SIZE: !Ref DiskSize
      Handler: "getSnapshot.handler"
      Role: !Sub ${SnapshotterLambdaRole.Arn}
      Runtime: python3.7
      Timeout: 30
  PrunerLambdaFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Code:
        S3Bucket: !Ref S3GethBucketName
        S3Key: lambdaPackage-31.zip
      Description: "Launch instances to snapshot chaindata"
      Environment:
        Variables:
          LAUNCH_TEMPLATE_ID: !Ref PrunerLaunchTemplate
          LAUNCH_TEMPLATE_VERSION: !Sub "${PrunerLaunchTemplate.LatestVersionNumber}"
          INSTANCE_TYPES: !Ref PrunerInstanceType
          SUBNET_ID:
            "Fn::ImportValue":
                !Sub "${InfrastructureStack}-PublicA"
          VOLUME_SIZE: !Ref DiskSize
      Handler: "getSnapshot.handler"
      Role: !Sub ${SnapshotterLambdaRole.Arn}
      Runtime: python3.7
      Timeout: 30

  SnapshotSchedulerRule:
    Type: AWS::Events::Rule
    Properties:
      Description: !Sub "Take a daily snapshot for the ${KafkaTopic} cluster"
      ScheduleExpression: !Ref SnapshotScheduleExpression
      Targets:
        - Arn: !Sub ${SnapshotterLambdaFunction.Arn}
          Id: !Sub "snapshot-${KafkaTopic}"

  SnapshotterInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Sub ${SnapshotterLambdaFunction.Arn}
      Action: 'lambda:InvokeFunction'
      Principal: events.amazonaws.com
      SourceArn: !Sub ${SnapshotSchedulerRule.Arn}

  DiskSizeLambdaRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
        Version: '2012-10-17'
  DiskSizeLambdaPolicy:
    Type: "AWS::IAM::Policy"
    Properties:
      Roles:
        - !Ref DiskSizeLambdaRole
      PolicyName: !Sub "DiskSizeLambdaPolicy${AWS::StackName}"
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action:
              - ec2:DescribeVolumes
              - ec2:ModifyVolume
            Resource: "*"
  DiskSizeLambdaFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Code:
        S3Bucket: !Ref S3GethBucketName
        S3Key: lambdaPackage-31.zip
      Description: "Align master volume sizes with CloudFormation parameters"
      Environment:
        Variables:
          VOLUME_SIZE: !Ref DiskSize
          VOLUME_NAME: !Sub "${KafkaTopic}-Master"
          ATTACHMENT_DEVICE: "/dev/sdf"
      Handler: "masterVolumeManager.sizeHandler"
      Role: !Sub ${DiskSizeLambdaRole.Arn}
      Runtime: python3.7
  DiskManagerSchedulerRule:
    Type: AWS::Events::Rule
    Properties:
      Description: Disk Size manager
      ScheduleExpression: "rate(15 minutes)"
      Targets:
        - Arn: !Sub ${DiskSizeLambdaFunction.Arn}
          Id: !Sub "disk-size-${KafkaTopic}"
  SnapDiskSizeLambdaFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Code:
        S3Bucket: !Ref S3GethBucketName
        S3Key: lambdaPackage-31.zip
      Description: "Align master volume sizes with CloudFormation parameters"
      Environment:
        Variables:
          VOLUME_SIZE: !Ref DiskSize
          VOLUME_NAME: !Sub "${AWS::StackName}-SnapshotVolume"
          ATTACHMENT_DEVICE: "/dev/sdf"
      Handler: "masterVolumeManager.sizeHandler"
      Role: !Sub ${DiskSizeLambdaRole.Arn}
      Runtime: python3.7
  SnapDiskManagerSchedulerRule:
    Type: AWS::Events::Rule
    Properties:
      Description: Disk Size manager
      ScheduleExpression: "rate(15 minutes)"
      Targets:
        - Arn: !Sub ${SnapDiskSizeLambdaFunction.Arn}
          Id: !Sub "disk-size-${KafkaTopic}"

  DiskManagerInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Sub ${DiskSizeLambdaFunction.Arn}
      Action: 'lambda:InvokeFunction'
      Principal: events.amazonaws.com
      SourceArn: !Sub ${DiskManagerSchedulerRule.Arn}
  SnapDiskManagerInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Sub ${SnapDiskSizeLambdaFunction.Arn}
      Action: 'lambda:InvokeFunction'
      Principal: events.amazonaws.com
      SourceArn: !Sub ${SnapDiskManagerSchedulerRule.Arn}

  SnapshotGCLambdaRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
        Version: '2012-10-17'
  SnapshotGCLambdaPolicy:
    Type: "AWS::IAM::Policy"
    Properties:
      Roles:
        - !Ref SnapshotGCLambdaRole
      PolicyName: !Sub "SnapshotGCLambdaPolicy${AWS::StackName}"
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action:
              - sns:Publish
              - ec2:DescribeSnapshots
              - ec2:DeleteSnapshot
            Resource: "*"

  SnapshotGCLambdaFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Code:
        S3Bucket: !Ref S3GethBucketName
        S3Key: lambdaPackage-22.zip
      Description: "Cleanup old chaindata snapshots"
      Environment:
        Variables:
          CLUSTER_ID: !Ref KafkaTopic
          SNAPSHOT_ID: !Ref SnapshotId
          SNS_TOPICS: !Sub "${AggregatedNotifications};${AlarmSNSTopic}"
      Handler: "gcSnapshot.handler"
      Role: !Sub ${SnapshotGCLambdaRole.Arn}
      Runtime: python3.7

  SnapshotGCSchedulerRule:
    Type: AWS::Events::Rule
    Properties:
      Description: !Sub "Cleanup old snapshots for the the ${KafkaTopic} cluster"
      ScheduleExpression: "rate(1 hour)"
      Targets:
        - Arn: !Sub ${SnapshotGCLambdaFunction.Arn}
          Id: !Sub "gc-${KafkaTopic}"

  SnapshotGCInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Sub ${SnapshotGCLambdaFunction.Arn}
      Action: 'lambda:InvokeFunction'
      Principal: events.amazonaws.com
      SourceArn: !Sub ${SnapshotGCSchedulerRule.Arn}

  RemoteMetricsLambdaFunction:
    Condition: HasRemoteRPCURL
    Type: "AWS::Lambda::Function"
    Properties:
      Code:
        S3Bucket: !Ref S3GethBucketName
        S3Key: lambdaPackage-22.zip
      Description: "Collect metrics for remote RPC URLs"
      Environment:
        Variables:
          CLUSTER_ID: !Ref KafkaTopic
          RPC_URL: !Ref RemoteRPCURL
      Handler: "remote_metrics.handler"
      Role: !Sub ${LogMetricsRole.Arn}
      Runtime: python3.7

  RemoteMetricsSchedulerRule:
    Condition: HasRemoteRPCURL
    Type: AWS::Events::Rule
    Properties:
      Description: !Sub "Remote metrics"
      ScheduleExpression: "rate(1 minute)"
      Targets:
        - Arn: !Sub ${RemoteMetricsLambdaFunction.Arn}
          Id: !Sub "remote-metrics-${KafkaTopic}"

  RemoteMetricsInvokePermission:
    Condition: HasRemoteRPCURL
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Sub ${RemoteMetricsLambdaFunction.Arn}
      Action: 'lambda:InvokeFunction'
      Principal: events.amazonaws.com
      SourceArn: !Sub ${RemoteMetricsSchedulerRule.Arn}

  VolumeGCLambdaRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
        Version: '2012-10-17'
  VolumeGCLambdaPolicy:
    Type: "AWS::IAM::Policy"
    Properties:
      Roles:
        - !Ref VolumeGCLambdaRole
      PolicyName: !Sub "VolumeGCLambdaPolicy${AWS::StackName}"
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action:
              - tag:GetResources
              - tag:TagResources
              - ec2:CreateTags
              - ec2:DescribeInstances
              - ec2:DeleteVolume
              - ec2:DetachVolume
              - ec2:DescribeVolumes
              - logs:PutLogEvents
              - logs:DescribeLogStreams
              - logs:DescribeLogGroups
              - logs:CreateLogStream
              - logs:CreateLogGroup
            Resource: "*"

  VolumeGCLambdaFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Code:
        S3Bucket: !Ref S3GethBucketName
        S3Key: lambdaPackage-26.zip
      Description: "Cleanup old chaindata snapshots"
      Environment:
        Variables:
          TAG_NAME: VOLUME_MGMT_GROUP
          TAG_VALUE: !Sub ${AWS::StackName}
          VOLUME_NAME: /dev/sdg
      Handler: "volumeGC.handler"
      Role: !Sub ${VolumeGCLambdaRole.Arn}
      Runtime: python3.7

  VolumeGCSchedulerRule:
    Type: AWS::Events::Rule
    Properties:
      EventPattern:
        source:
          -  "aws.ec2"
        detail-type:
          - "EC2 Instance State-change Notification"
      State: ENABLED
      Targets:
        - Arn: !Sub ${VolumeGCLambdaFunction.Arn}
          Id: !Sub "vgc-${KafkaTopic}"

  VolumeGCInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Sub ${VolumeGCLambdaFunction.Arn}
      Action: 'lambda:InvokeFunction'
      Principal: events.amazonaws.com
      SourceArn: !Sub ${VolumeGCSchedulerRule.Arn}

  TransactionOffsetLag:
    Condition: HasMSKName
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmDescription: "Warns when transactions are not beign relayed"
      ComparisonOperator: "GreaterThanThreshold"
      Dimensions:
        - Name: Cluster Name
          Value: !Ref MSKClusterName
        - Name: Consumer Group
          Value: !Sub "${KafkaTopic}-cg"
        - Name: Topic
          Value: !Sub "${NetworkId}-tx"
      AlarmActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      InsufficientDataActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      EvaluationPeriods: 4
      DatapointsToAlarm: 4
      MetricName: "MaxOffsetLag"
      Namespace: AWS/Kafka
      OKActions:
        - !Ref AggregatedNotifications
        - !If [ HasSNSTopic, !Ref AlarmSNSTopic,  !Ref 'AWS::NoValue']
        - !If [ HasUrgentWebhook, !Ref UrgentNotifications, !Ref 'AWS::NoValue' ]
      Period: 60
      Statistic: Maximum
      Threshold: 20
      TreatMissingData: ignore

  CloudwatchDashboard:
    Type: AWS::CloudWatch::Dashboard
    Properties:
      DashboardName: !Sub "${AWS::StackName}-${AWS::Region}"
      DashboardBody:
        Fn::Sub:
          - |
              {
                "widgets": [
                    {
                        "type": "metric",
                        "x": 0,
                        "y": 0,
                        "width": 6,
                        "height": 6,
                        "properties": {
                            "metrics": [
                                [ "BlockData", "number", "clusterId", "${KafkaTopic}", { "label": "Master Block Number" } ],
                                [ "ReplicaData", "num",  "clusterId", "${KafkaTopic}", { "stat": "Average", "label": "Replica Block Number (avg)" } ],
                                [ "ReplicaData", "num",  "clusterId", "${KafkaTopic}", { "stat": "Minimum", "label": "Replica Block Number (min)" } ]
                            ],
                            "view": "timeSeries",
                            "stacked": false,
                            "region": "${AWS::Region}",
                            "title": "Block Number"
                        }
                    },
                    {
                        "type": "metric",
                        "x": 12,
                        "y": 0,
                        "width": 6,
                        "height": 6,
                        "properties": {
                            "view": "timeSeries",
                            "stacked": false,
                            "metrics": [
                                [ "BlockData", "age", "clusterId", "${KafkaTopic}" ],
                                [ "ReplicaData", "age",  "clusterId", "${KafkaTopic}", { "stat": "Average", "label": "Replica Block Age (avg)" } ],
                                [ "ReplicaData", "age",  "clusterId", "${KafkaTopic}", { "stat": "Maximum", "label": "Replica Block Age (max)" } ]
                            ],
                            "region": "${AWS::Region}",
                            "title": "Block Age"
                        }
                    },
                    {
                        "type": "metric",
                        "x": 18,
                        "y": 0,
                        "width": 6,
                        "height": 6,
                        "properties": {
                            "view": "timeSeries",
                            "stacked": false,
                            "metrics": [
                                [ "ReplicaData", "offsetAge", "clusterId", "${KafkaTopic}" ]
                            ],
                            "region": "${AWS::Region}",
                            "title": "Offset Age"
                        }
                    },
                    {
                        "type": "metric",
                        "x": 6,
                        "y": 6,
                        "width": 6,
                        "height": 6,
                        "properties": {
                            "view": "timeSeries",
                            "stacked": false,
                            "metrics": [
                                [ "CWAgent", "disk_used_percent", "path", "/var/lib/ethereum", "AutoScalingGroupName", "${MasterAutoScalingGroup}", "device", "nvme1n1", "fstype", "ext4" ],
                                [ "CWAgent", "disk_used_percent", "path", "/var/lib/ethereum", "AutoScalingGroupName", "${MasterAutoScalingGroup}", "device", "xvdf", "fstype", "ext4" ],
                                [ "CWAgent", "disk_used_percent", "path", "/var/lib/ethereum", "AutoScalingGroupName", "${MasterAutoScalingGroup}", "fstype", "ext4" ]
                            ],
                            "region": "${AWS::Region}",
                            "title": "Disk Usage"
                        }
                    },
                    {
                        "type": "metric",
                        "x": 0,
                        "y": 6,
                        "width": 6,
                        "height": 6,
                        "properties": {
                            "view": "timeSeries",
                            "stacked": false,
                            "metrics": [
                                [ "AWS/EC2", "CPUUtilization", "AutoScalingGroupName", "${MasterAutoScalingGroup}", { "stat": "Average", "label": "Master" }],
                                [ "AWS/EC2", "CPUUtilization", "AutoScalingGroupName", "${ReplicaAutoScalingGroup}", { "stat": "Average", "label": "Replicas (avg)" } ],
                                [ "AWS/EC2", "CPUUtilization", "AutoScalingGroupName", "${ReplicaAutoScalingGroup}", { "stat": "Maximum", "label": "Replicas (max)" } ]
                            ],
                            "region": "${AWS::Region}",
                            "title": "CPU Utilization"
                        }
                    },
                    {
                        "type": "metric",
                        "x": 6,
                        "y": 0,
                        "width": 6,
                        "height": 6,
                        "properties": {
                            "metrics": [
                                [ { "expression": "m1-m2", "label": "Expression1", "id": "e1" } ],
                                [ "BlockData", "number", "clusterId", "${KafkaTopic}", { "id": "m1", "visible": false } ],
                                [ "ReplicaData", "num", "clusterId", "${KafkaTopic}", { "id": "m2", "visible": false } ]
                            ],
                            "view": "timeSeries",
                            "stacked": false,
                            "region": "${AWS::Region}",
                            "title": "Block Lag"
                        }
                    },
                    {
                        "type": "metric",
                        "x": 12,
                        "y": 6,
                        "width": 6,
                        "height": 6,
                        "properties": {
                            "view": "timeSeries",
                            "stacked": false,
                            "metrics": [
                                [ "CWAgent", "mem_used_percent", "AutoScalingGroupName", "${MasterAutoScalingGroup}" , { "stat": "Average", "label": "Master" }],
                                [ "CWAgent", "mem_used_percent", "AutoScalingGroupName", "${ReplicaAutoScalingGroup}" , { "stat": "Average", "label": "Replicas (avg)" } ],
                                [ "CWAgent", "mem_used_percent", "AutoScalingGroupName", "${ReplicaAutoScalingGroup}" , { "stat": "Maximum", "label": "Replicas (max)" } ]
                            ],
                            "region": "${AWS::Region}",
                            "title": "Memory Utilization"
                        }
                    },
                    {
                        "type": "metric",
                        "x": 6,
                        "y": 12,
                        "width": 6,
                        "height": 6,
                        "properties": {
                            "view": "timeSeries",
                            "stacked": false,
                            "metrics": [
                                [ "BlockData", "peerCount", "clusterId", "${KafkaTopic}" ]
                            ],
                            "region": "${AWS::Region}",
                            "title": "Master Peer Count"
                        }
                    },
                    {
                        "type": "metric",
                        "properties": {
                            "view": "timeSeries",
                            "stacked": false,
                            "metrics": [
                                [ "ReplicaData", "delta",  "clusterId", "${KafkaTopic}", { "stat": "Average", "label": "Replica Delta (avg)" } ],
                                [ "ReplicaData", "delta",  "clusterId", "${KafkaTopic}", { "stat": "Maximum", "label": "Replica Delta (max)" } ]
                            ],
                            "region": "${AWS::Region}",
                            "title": "Replication Delta"
                        }
                    },
                    {
                        "type": "metric",
                        "properties": {
                            "view": "timeSeries",
                            "stacked": false,
                            "metrics": [
                                [ "ReplicaData", "concurrency",  "clusterId", "${KafkaTopic}", { "stat": "Average", "label": "Concurrent Requests (avg)" } ],
                                [ "ReplicaData", "concurrency",  "clusterId", "${KafkaTopic}", { "stat": "Maximum", "label": "Concurrent Requests (max)" } ]
                            ],
                            "region": "${AWS::Region}",
                            "title": "Replica Concurrency"
                        }
                    },
                    {
                        "type": "metric",
                        "properties": {
                            "view": "timeSeries",
                            "stacked": false,
                            "metrics": [
                                [ "ReplicaData", "backend_error",  "clusterId", "${KafkaTopic}", { "stat": "Sum", "label": "Backend Errors (avg)" } ]
                            ],
                            "region": "${AWS::Region}",
                            "title": "Backend Errors"
                        }
                    }${DashExt}
                ]
              }
          - DashExt: !If [ HasDashboardExtensions, {"Fn::ImportValue": !Ref DashboardExtension}, "" ]
